# -*- coding: utf-8 -*-
"""Deep Learning PyTorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CYxbg-lGWZk_MDRsQ4sWvsENrwy0DXZe

# Multiple Linear Regression
"""

from torch import nn
import torch
torch.manual_seed(1)

w = torch.tensor([[2.0],[3.0]],requires_grad=True)
b = torch.tensor([[1.0]],requires_grad=True)

def forward(x):
  return torch.mm(w,x)+b

x = torch.tensor([[1.0,2.0]])
yhat = forward(x)
print("The result: ", yhat)

X = torch.tensor([[[1.0,1.0],[1.0,2.0],[1.0,3.0]]])
# yhat = forward(X)

model = nn.Linear(in_features=2,out_features=1)
yhat = model(x)

# build custom model
class linear_regression(nn.Module):

  def __init__(self,input_size,output_size):
    super(linear_regression,self).__init__()
    self.linear = nn.Linear(input_size,output_size)

  def forward(self,x):
    yhat = self.linear(x)
    return yhat


model = linear_regression(2,1)
print("The parameters: ", list(model.parameters()))
print("The parameters: ", model.state_dict())
yhat = model(x)
print("The result: ", yhat)

# # 將data傳入model進行forward propagation
# # 計算loss
# # 清空前一次的gradient
# # 根據loss進行back propagation，計算gradient
# # 做gradient descent
# output = model(batch_x)
# loss = criterion(output, batch_y)
# optimizer.zero_grad()
# loss.backward()
# optimizer.step()

# Pytorch不幫你自動清空gradient，而是要你呼叫optimizer.zero_grad()來做這件事是因為，這樣子你可以有更大的彈性去做一些黑魔法，畢竟，誰規定每一次iteration都要清空gradient?

# 試想你今天GPU的資源就那麼小，可是你一定要訓練一個很大的model，然後如果batch size不大又train不起來，那這時候該怎麼辦?

# 雖然沒有課金解決不了的事情，如果有，那就多課一點…不是，這邊提供另外一種設計思維:

# 你可以將你的model每次都用小的batch size去做forward/backward，但是在update的時候是多做幾個iteration在做一次。

# 這個想法就是梯度累加(gradient accumulation)，也就是說我們透過多次的iteration累積backward的loss，然後只對應做了一次update，間接的做到了大batch size時候的效果。

# for idx, (batch_x, batch_y) in enumerate(data_loader):
#     output = model(batch_x)
#     loss = criterion(output, batch_y)

#     loss = loss / accumulation_step
#     loss.backward()

#     if (idx % accumulation_step) == 0:
#         optimizer.step() # update
#         optimizer.zero_grad() # reset

from torch import nn,optim
import torch
import numpy as np
import matplotlib.pyplot as plt
import torch.optim
from mpl_toolkits.mplot3d import Axes3D
from torch.utils.data import Dataset,DataLoader

def Plot_2D_Plane(model,dataset,n=0):
  print(model.state_dict())
  w1 = model.state_dict()['linear.weight'].numpy()[0][0]
  w2 = model.state_dict()['linear.weight'].numpy()[0][1]
  b = model.state_dict()['linear.bias'].numpy()

  x1 = dataset.x[:,0].view(-1,1).numpy() #-1代表自動判斷維度
  x2 = dataset.x[:,1].view(-1,1).numpy()
  y = dataset.y.numpy()

  X,Y = np.meshgrid(np.arange(x1.min(),x1.max(),0.05),np.arange(x2.min(),x2.max(),0.05))
  yhat = w1*X + w2*Y +b

  fig = plt.figure()
  ax =  fig.add_subplot(projection='3d')
  ax.plot(x1[:,0],x2[:,0],y[:,0],'ro',label='y')
  ax.plot_surface(X,Y,yhat)

  ax.set_xlabel('x1')
  ax.set_ylabel('x2')
  ax.set_zlabel('y')
  plt.title('estimated plane iteration:' + str(n))
  ax.legend()

  plt.show()

class Data2D(Dataset):
  def __init__(self):
    self.x = torch.zeros(20,2)
    self.x[:,0] = torch.arange(-1,1,0.1)
    self.x[:,1] = torch.arange(-1,1,0.1)
    self.w = torch.tensor([[1.0],[1.0]])
    self.b  =1
    self.f = torch.mm(self.x,self.w) + self.b
    self.y = self.f + 0.1* torch.randn((self.x.shape[0],1))
    self.len = self.x.shape[0]

  def __getitem__(self,index):
    return self.x[index],self.y[index]

  def __len__(self):
    return self.len

data_set = Data2D()

class linear_regression(nn.Module):
  def __init__(self,input_size,output_size):
    super(linear_regression,self).__init__()
    self.linear = nn.Linear(in_features = input_size,out_features=output_size)

  def forward(self,x):
    yhat = self.linear(x)
    return yhat

model = linear_regression(2,1)
optimizer = optim.SGD(model.parameters(),lr=0.1)
criterion = nn.MSELoss()
train_loader = DataLoader(dataset=data_set, batch_size=2)

LOSS = []
print("Before Training: ")
Plot_2D_Plane(model, data_set)
epochs = 100

def train_model(epochs):
  for epoch in range(epochs):
    for x,y in train_loader:
      yhat = model(x)
      loss = criterion(yhat,y)
      LOSS.append(loss)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

train_model(epochs)
print("After Training: ")
Plot_2D_Plane(model, data_set, epochs)
with torch.no_grad():
  plt.plot(LOSS)
  plt.xlabel("Iterations ")
  plt.ylabel("Cost/total loss ")
  plt.show()

from torch import nn
import torch
torch.manual_seed(1)

class linear_regression(nn.Module):
  def __init__(self,input_size,output_size):
    super(linear_regression,self).__init__()
    self.linear = nn.Linear(input_size,output_size)

  def forward(self,x):
    return self.linear(x)


model = linear_regression(1,10)
model(torch.tensor([1.0]))
print(list(model.parameters()))

x=torch.tensor([[1.0]])
print(model(x))

X=torch.tensor([[1.0],[1.0],[3.0]])
print(model(X))

import torch
import numpy as np
import matplotlib.pyplot as plt
from torch import optim,nn
from mpl_toolkits.mplot3d import Axes3D
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms

torch.manual_seed(1)
class Data(Dataset):
  def __init__(self):
    self.x = torch.zeros(20,2)
    self.x[:,0] = torch.arange(-1,1,0.1)
    self.x[:,1] = torch.arange(-1,1,0.1)
    self.w = torch.tensor([ [1.0,-1.0],[1.0,3.0]])
    self.b = torch.tensor([[1.0,-1.0]])
    self.f = torch.mm(self.x,self.w)+self.b
    self.y = self.f+0.001*torch.randn((self.x.shape[0],1))
    self.len=self.x.shape[0]

  def __getitem__(self,index):
    return self.x[index] , self.y[index]

  def __len__(self):
    return self.len

data_set=Data()
class linear_regression(nn.Module):
  def __init__(self,input_size,output_size):
    super(linear_regression,self).__init__()
    self.linear = nn.Linear(input_size,output_size)

  def forward(self,x):
    return self.linear(x)

model=linear_regression(2,2)
optimizer = optim.SGD(model.parameters(),lr = 0.1)
criterion = nn.MSELoss()
train_loader=DataLoader(dataset=data_set,batch_size=5)

LOSS = []
epochs = 100
for epoch in range(epochs):
  for x,y in train_loader:
    yhat = model(x)
    loss = criterion(yhat,y)
    LOSS.append(loss)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
with torch.no_grad():
  plt.plot(LOSS)
  plt.xlabel("iterations ")
  plt.ylabel("Cost/total loss ")
  plt.show()



"""# Logistic Regression"""

import torch.nn as nn
import torch
import matplotlib.pyplot as plt
import numpy as np

torch.manual_seed(2)
z = torch.arange(-100,100,0.1).view(-1,1) #view的作用是reshape
sig = nn.Sigmoid()
yhat = sig(z)

plt.plot(z.numpy(), yhat.numpy())
plt.xlabel('z')
plt.ylabel('yhat')

x = torch.tensor([[1.0]])
X = torch.tensor([[1.0], [100]])
print('x = ', x)
print('X = ', X)

model = nn.Sequential(nn.Linear(1,1),nn.Sigmoid())
print("list(model.parameters()):\n ", list(model.parameters()))
print("\nmodel.state_dict():\n ", model.state_dict())
yhat = model(x)
print("The prediction: ", yhat)
yhat = model(X)
print("The prediction: ", yhat)

x = torch.tensor([[1.0, 1.0]])
X = torch.tensor([[1.0, 1.0], [1.0, 2.0], [1.0, 3.0]])
print('x = ', x)
print('X = ', X)
model = nn.Sequential(nn.Linear(2, 1), nn.Sigmoid())
print("list(model.parameters()):\n ", list(model.parameters()))
print("\nmodel.state_dict():\n ", model.state_dict())
yhat = model(x)
print("The prediction: ", yhat)
yhat = model(X)
print("The prediction: ", yhat)

class logistic_regression(nn.Module):
  def __init__(self,n_inputs):
    super(logistic_regression,self).__init__()
    self.linear = nn.Linear(n_inputs,1)

  def forward(self,x):
    return self.linear(x)

x = torch.tensor([[1.0]])
X = torch.tensor([[-100], [0], [100.0]])
print('x = ', x)
print('X = ', X)
model = logistic_regression(2)
print("list(model.parameters()):\n ", list(model.parameters()))
print("\nmodel.state_dict():\n ", model.state_dict())
yhat = model(x)
print("The prediction result: \n", yhat)
yhat = model(X)
print("The prediction result: \n", yhat)

x = torch.tensor([[1.0, 2.0]])
X = torch.tensor([[100, -100], [0.0, 0.0], [-100, 100]])
print('x = ', x)
print('X = ', X)
yhat = model(x)
print("The prediction result: \n", yhat)
yhat = model(X)
print("The prediction result: \n", yhat)

"""## Logistic Regression and Bad Initialization Value"""

import numpy as np
import matplotlib.pyplot as plt
import torch
from mpl_toolkits import mplot3d
from torch.utils.data import Dataset,DataLoader
import torch.nn as nn

class plot_error_surfaces(object):
  def __init__(self,w_range,b_range,X,Y,n_samples=30,go=True):
    W = np.linspace(-w_range,w_range,n_samples)
    B = np.linspace(-b_range,b_range,n_samples)
    w,b = np.meshgrid(W,B)
    Z = np.zeros((30,30))
    count1 = 0
    self.y = Y.numpy()
    self.x = X.numpy()
    for w1,b1 in zip(w,b):
      count2 = 0
      for w2,b2 in zip(w1,b1):
        Z[count1, count2] = np.mean((self.y - (1 / (1 + np.exp(-1*w2 * self.x - b2)))) ** 2)
        count2 += 1
      count1+=1
    self.Z=Z
    self.w = w
    self.b = b
    self.W = []
    self.B = []
    self.LOSS = []
    self.n = 0
    if go==True:
      plt.figure()
      plt.figure(figsize = (7.5,5))
      plt.axes(projection='3d').plot_surface(self.w,self.b,self.Z,rstride=1,cstride=1,cmap='viridis',edgecolor='none')
      plt.title('Loss Surface')
      plt.xlabel('w')
      plt.ylabel('b')
      plt.show()
      plt.figure()
      plt.title('Loss Surface Contour')
      plt.xlabel('w')
      plt.ylabel('b')
      plt.contour(self.w,self.b,self.Z)
      plt.show()

  def set_para_loss(self,model,loss):
    self.n = self.n+1
    self.W.append(list(model.parameters())[0].item())
    self.B.append(list(model.parameters())[1].item())
    self.LOSS.append(loss)

  def final_plot(self):
    ax= plt.axes(projection='3d')
    ax.plot_wireframe(self.w,self.b,self.Z)
    ax.scatter(self.W,self.B,self.LOSS,c='r',marker='x',s=200,alpha=1)
    plt.figure()
    plt.contour(self.w,self.b,self.Z)
    plt.scatter(self.W,self.B,c='r',marker='x')
    plt.xlabel('w')
    plt.ylabel('b')
    plt.show()

  def plot_ps(self):
    plt.subplot(121)
    plt.ylim
    plt.plot(self.x,self.y,'ro',label='training points')
    plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label="estimated line")
    plt.plot(self.x, 1 / (1 + np.exp(-1 * (self.W[-1] * self.x + self.B[-1]))), label='sigmoid')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.ylim((-0.1,2))
    plt.title('Data Space Iteration: ' + str(self.n))
    plt.show()
    plt.subplot(122)
    plt.contour(self.w, self.b, self.Z)
    plt.scatter(self.W, self.B, c='r', marker='x')
    plt.title('Loss Surface Contour Iteration' + str(self.n))
    plt.xlabel('w')
    plt.ylabel('b')

def PlotStuff(X,Y,model,epoch,leg=True):
  plt.plot(X.numpy(),model(X).detach().numpy(),label=('epoch'+str(epoch)))
  plt.plot(X.numpy(),Y.numpy(),'r')
  if leg==True:
    plt.legend()
  else:
    pass

torch.manual_seed(0)
class Data(Dataset):
  def __init__(self):
    self.x = torch.arange(-1, 1, 0.1).view(-1, 1)
    self.y = torch.zeros(self.x.shape[0], 1)
    self.y[self.x[:, 0] > 0.2] = 1
    self.len = self.x.shape[0]

  def __getitem__(self,index):
    return self.x[index],self.y[index]

  def __len__(self):
    return self.len

data_set = Data()

class logistic_regression(nn.Module):
  def __init__(self,n_inputs):
    super(logistic_regression,self).__init__()
    self.linear = nn.Linear(n_inputs,1)

  def forward(self,x):
    yhat = torch.sigmoid(self.linear(x))
    return yhat

model = logistic_regression(1)
model.state_dict()['linear.weight'].data[0] = torch.tensor([[-5]])
model.state_dict()['linear.bias'].data[0] = torch.tensor([[-10]])
print("The parameters: ", model.state_dict())

get_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1], 30)
trainloader = DataLoader(dataset=data_set, batch_size=3)
criterion_rms = nn.MSELoss()
learning_rate = 2
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
def train_model(epochs):
  for epoch in range(epochs):
    for x, y in trainloader:
      yhat = model(x)
      loss = criterion_rms(yhat, y)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      get_surface.set_para_loss(model, loss.tolist())
    if epoch % 20 == 0:
      get_surface.plot_ps()

train_model(100)
yhat = model(data_set.x)
label = yhat > 0.5
print("The accuracy: ", torch.mean((label == data_set.y.type(torch.ByteTensor)).type(torch.float)))

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
def criterion(yhat,y):
  out = -1 * torch.mean(y * torch.log(yhat) + (1 - y) * torch.log(1 - yhat))
  return out

# Build in criterion
# criterion = nn.BCELoss()

trainloader = DataLoader(dataset = data_set, batch_size = 3)
learning_rate = 2
optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)
def train_model(epochs):
  for epoch in range(epochs):
    for x, y in trainloader:
      yhat = model(x)
      loss = criterion(yhat, y)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      get_surface.set_para_loss(model, loss.tolist())
    if epoch % 20 == 0:
      get_surface.plot_ps()

train_model(100)
yhat = model(data_set.x)
label = yhat > 0.5
print("The accuracy: ", torch.mean((label == data_set.y.type(torch.ByteTensor)).type(torch.float)))

"""# Deep Neural Network for Breast Cancer Classification"""

!pip install ucimlrepo==0.0.7

from ucimlrepo import fetch_ucirepo
breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)

# data (as pandas dataframes)
X = breast_cancer_wisconsin_diagnostic.data.features
y = breast_cancer_wisconsin_diagnostic.data.targets

import pandas as pd
data = pd.concat([X,y],axis=1)
data_B = data[data['Diagnosis'] == 'B']
data_M = data[data['Diagnosis'] == 'M']

data_B = data_B.sample(n=200, random_state=42)
data_M = data_M.sample(n=200, random_state=42)
balanced_data = pd.concat([data_B, data_M])

display(balanced_data['Diagnosis'].value_counts())

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import torch
X = balanced_data.drop('Diagnosis', axis=1)
y = balanced_data['Diagnosis']
y = y.map({'B': 0, 'M': 1})

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
from torch.utils.data import DataLoader, TensorDataset
scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

X_train = torch.tensor(X_train, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_train = torch.tensor(y_train.values, dtype=torch.long)
y_test = torch.tensor(y_test.values, dtype=torch.long)

train_dataset = TensorDataset(X_train, y_train)
test_dataset = TensorDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)

import torch.nn as nn
class ClassificationNet(nn.Module):
  def __init__(self,input_units = 30,hidden_units=64,output_units=2):
    super(ClassificationNet,self).__init__()
    self.fc1 = nn.Linear(input_units,hidden_units)
    self.fc2 = nn.Linear(hidden_units,output_units)

  def forward(x):
    x = torch.relu(self.fc1(x))
    x = self.fc2(x)
    return x

model = ClassificationNet(input_units=30, hidden_units=64, output_units=2)
import torch.optim as optim

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
epochs = 10
train_losses = []
test_losses = []

for epoch in range(epochs):
  model.train()
  running_loss = 0.0
  for X_batch,y_batch in train_loader:
    optimizer.zero_grad()
    outputs = model(X_batch)
    loss = criterion(outputs, y_batch)
    loss.backward()
    optimizer.step()
    running_loss += loss.item()
  train_loss = running_loss/len(train_loader)
  train_losses.append(train_loss)
  model.eval()
  test_loss = 0.0
  with torch.no_grad():
    for X_batch, y_batch in test_loader:
      test_outputs = model(X_batch)
      loss = criterion(test_outputs, y_batch)
      test_loss += loss.item()

  test_loss /= len(test_loader)
  test_losses.append(test_loss)

import matplotlib.pyplot as plt

# Plot the loss curves
plt.figure(figsize=(10, 6))
plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')
plt.plot(range(1, epochs + 1), test_losses, label='Test Loss', linestyle='--')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Test Loss Curve')
plt.legend()
plt.grid(True)
plt.show()

"""# Softmax Classifier 1"""

import torch.nn as nn
import torch
import matplotlib.pyplot as plt
import numpy as np
from torch.utils.data import Dataset,DataLoader

def plot_data(data_set,model=None,n=1,color=False):
  X = data_set[:][0]
  Y = data_set[:][1]
  plt.plot(X[Y==0,0].numpy(),Y[Y==0].numpy(),'bo',label = 'y = 0')
  plt.plot(X[Y==1,0].numpy(),0 * Y[Y == 1].numpy(), 'ro', label = 'y = 1')
  plt.plot(X[Y == 2, 0].numpy(), 0 * Y[Y == 2].numpy(), 'go', label = 'y = 2')
  plt.ylim((-0.1,3))
  plt.legend()
  if model != None:
    w = list(model.parameters())[0][0].detach()
    b = list(model.parameters())[1][0].detach()
    y_label = ['yhat=0', 'yhat=1', 'yhat=2']
    y_color = ['b','r','g']
    Y = []
    for w,b,y_l,y_c in zip(model.state_dict()['0.weight'],model.state_dict()['0.bias'],y_label,y_color):
      Y.append((w*X+b).numpy())
      plt.plot(X.numpy(),(w*X+b).numpy(),y_c,label=y_l)
    if color==True:
      x = X.numpy()
      x = x.reshape(-1)
      top = np.ones(x.shape)
      y0 = Y[0].reshape(-1)
      y1 = Y[1].reshape(-1)
      y2 = Y[2].reshape(-1)
      plt.fill_between(x, y0, where = y1 > y1, interpolate = True, color = 'blue')
      plt.fill_between(x, y0, where = y1 > y2, interpolate = True, color = 'blue')
      plt.fill_between(x, y1, where = y1 > y0, interpolate = True, color = 'red')
      plt.fill_between(x, y1, where = ((y1 > y2) * (y1 > y0)),interpolate = True, color = 'red')
      plt.fill_between(x, y2, where = (y2 > y0) * (y0 > 0),interpolate = True, color = 'green')
      plt.fill_between(x, y2, where = (y2 > y1), interpolate = True, color = 'green')
  plt.legend()
  plt.show()

torch.manual_seed(0)
class Data(Dataset):
  def __init__(self):
    self.x = torch.arange(-2,2,0.1).view(-1,1)
    self.y = torch.zeros(self.x.shape[0])
    self.y[(self.x > -1.0)[:, 0] * (self.x < 1.0)[:, 0]] = 1
    self.y[(self.x >= 1.0)[:, 0]] = 2
    self.y = self.y.type(torch.LongTensor)
    self.len = self.x.shape[0]

  def __getitem__(self,index):
    return self.x[index],self.y[index]

  def __len__(self):
    return self.len

data_set = Data()
plot_data(data_set)

model = nn.Sequential(nn.Linear(1,3))
model.state_dict()

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(),lr=0.01)
train_loader = DataLoader(dataset = data_set, batch_size = 5)
LOSS = []
def train_model(epochs):
  for epoch in range(epochs):
    if epoch % 50 ==0:
      pass
      plot_data(data_set,model)
    for x,y in train_loader:
      optimizer.zero_grad()
      yhat = model(x)
      loss = criterion(yhat,y)
      LOSS.append(loss)
      loss.backward()
      optimizer.step()

train_model(300)
z =  model(data_set.x)
_, yhat = z.max(1)
print("The prediction:", yhat)

correct = (data_set.y == yhat).sum().item()
accuracy = correct / len(data_set)
print("The accuracy: ", accuracy)

Softmax_fn=nn.Softmax(dim=-1)
Probability =Softmax_fn(z)
for i in range(3):
    print("probability of class {} isg given by  {}".format(i, Probability[0,i]) )

"""# Softmax Mnist dataset"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import matplotlib.pyplot as plt
import numpy as np

def PlotParameters(model):
  W = model.state_dict()['linear.weight'].data
  w_min = W.min().item()
  w_max = W.max().item()
  fig,axes = plt.subplots(2,5)
  fig.subplots_adjust(hspace=0.01, wspace=0.1)

  for i,ax in enumerate(axes.flat):
    if i<10:
      ax.set_xlabel("class: {0}".format(i))
      ax.imshow(W[i,:].view(28,28),vmin=w_min,vmax=w_max,cmap='seismic')
      ax.set_xticks([])
      ax.set_yticks([])
  plt.show()

def show_data(data_sample):
  plt.imshow(data_sample[0].numpy().reshape(28,28),cmap='gray')
  plt.title('y = ' + str(data_sample[1]))

train_dataset = dsets.MNIST(root='./data',train=True,download=True,transform=transforms.ToTensor())
validation_dataset = dsets.MNIST(root='./data',download=True,transform=transforms.ToTensor())
print("Type of data element: ", type(train_dataset[0][1]))

print("The image: ", show_data(train_dataset[3]))
print("The label: ", train_dataset[3][1])

# build softmax classifer
class SoftMax(nn.Module):
  def __init__(self,input_size,output_size):
    super(SoftMax,self).__init__()
    self.linear = nn.Linear(input_size,output_size)

  def forward(self,x):
    z = self.linear(x)
    return z

train_dataset[0][0].shape

input_dim = 28*28
output_dim = 10
model = SoftMax(input_dim,output_dim)
print('W: ',list(model.parameters())[0].size())
print('b: ',list(model.parameters())[1].size())
PlotParameters(model)

learning_rate=0.1
optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)
criterion = nn.CrossEntropyLoss()
train_loader = torch.utils.data.DataLoader(dataset = train_dataset,batch_size=100)
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=5000)

n_epochs=10
loss_list = []
acc_list = []
N_test = len(validation_dataset)
def train_model(n_epochs):
  for epoch in range(n_epochs):
    for x,y in train_loader:
      optimizer.zero_grad()
      z = model(x.view(-1,28*28))
      loss = criterion(z,y)
      loss.backward()
      optimizer.step()
    correct = 0
    for x_test, y_test in validation_loader:
      z = model(x_test.view(-1,28*28))
      _,yhat= torch.max(z.data,1)
      correct += (yhat == y_test).sum().item()
    accuracy = correct / N_test
    loss_list.append(loss.data)
    acc_list.append(accuracy)

train_model(n_epochs)
fig, ax1 = plt.subplots()
color = 'tab:red'
ax1.plot(loss_list,color=color)
ax1.set_xlabel('epoch',color=color)
ax1.set_ylabel('total loss',color=color)
ax1.tick_params(axis='y', color=color)

ax2 = ax1.twinx()
color = 'tab:blue'
ax2.set_ylabel('accuracy', color=color)
ax2.plot( acc_list, color=color)
ax2.tick_params(axis='y', color=color)
fig.tight_layout()

PlotParameters(model)
Softmax_fn=nn.Softmax(dim=-1)
count = 0
for x, y in validation_dataset:
  z = model(x.reshape(-1, 28 * 28))
  _, yhat = torch.max(z, 1)
  if yhat != y:
    show_data((x, y))
    plt.show()
    print("yhat:", yhat)
    print("probability of class ", torch.max(Softmax_fn(z)).item())
    count += 1
  if count >= 5:
    break

"""# Simple One Hidden Layer Neural Network"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from torch import sigmoid
torch.manual_seed(0)

def PlotStuff(X,Y,model,epoch,leg=True):
  plt.plot(X.numpy(),model(X).detach().numpy(),label=('epoch '+str(epoch)))
  plt.plot(X.numpy(),Y.numpy(),'r')
  plt.xlabel('x')
  if leg == True:
    plt.legend()
  else:
    pass

class Net(nn.Module):
  def __init__(self,D_in,H,D_out):
    super(Net,self).__init__()
    self.linear1 = nn.Linear(D_in,H)
    self.linear2 = nn.Linear(H,D_out)
    self.a1 = None
    self.l1 = None
    self.l2 = None

  def forward(self,x):
    self.l1 = self.linear1(x)
    self.a1 = sigmoid(self.l1)
    self.l2 = self.linear2(self.a1)
    yhat = sigmoid(self.l2)
    return yhat

def train(Y,X,model,optimizer,criterion,epochs=1000):
  cost = []
  total = 0
  for epoch in range(epochs):
    total = 0
    for y,x in zip(Y,X):
      yhat = model(x)
      loss = criterion(yhat,y)
      loss.backward()
      optimizer.step()
      optimizer.zero_grad()
      total+=loss.item()
    cost.append(total)
    if epoch % 300 == 0:
      PlotStuff(X,Y,model,epoch,leg=True)
      plt.show()
      model(X)
      plt.scatter(model.a1.detach().numpy()[:,0],model.a1.detach().numpy()[:,1],c=Y.numpy().reshape(-1))
      plt.title('activations')
      plt.show()
  return cost

X = torch.arange(-20, 20, 1).view(-1, 1).type(torch.FloatTensor)
Y = torch.zeros(X.shape[0])
Y[(X[:, 0] > -4) & (X[:, 0] < 4)] = 1.0

def criterion_cross(outputs, labels): # like bce loss
    out = -1 * torch.mean(labels * torch.log(outputs) + (1 - labels) * torch.log(1 - outputs))
    return out

D_in = 1
H = 2
D_out = 1
learning_rate = 0.1
model = Net(D_in,H,D_out)
optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)
cost_cross = train(Y,X,model,optimizer,criterion_cross,epochs=1000)
plt.plot(cost_cross)
plt.xlabel('epoch')
plt.title('cross entropy loss')

x=torch.tensor([0.0])
yhat=model(x)
print(yhat)
X_=torch.tensor([[0.0],[2.0],[3.0]])
Yhat=model(X_)
print(Yhat)
Yhat=Yhat>0.5
Yhat

"""# Neural Networks More Hidden Neurons"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset,DataLoader

def get_hist(model,data_set):
  activations=model.activation(data_set.x)
  for i,activation in enumerate(activations):
    plt.hist(activation.numpy(),4,density=True)
    plt.title("Activation layer " + str(i+1))
    plt.xlabel("Activation")
    plt.xlabel("Activation")
    plt.legend()
    plt.show()

def PlotStuff(X,Y,model=None,leg=False):
  plt.plot(X[Y==0].numpy(),Y[Y==0].numpy(),'or',label='training points y=0')
  plt.plot(X[Y==1].numpy(),Y[Y==1].numpy(),'ob',label='training points y=1')

  if model != None:
    plt.plot(X.numpy(),model(X).detach().numpy(),label='neural network')
  plt.legend()
  plt.show()

class Data(Dataset):
  def __init__(self):
    self.x = torch.linspace(-20,20,100).view(-1,1)
    self.y = torch.zeros(self.x.shape[0])
    self.y[(self.x[:,0]>-10)&(self.x[:,0]<-5)]=1
    self.y[(self.x[:,0]>5)& (self.x[:,0]<10)]=1
    self.y = self.y.view(-1,1)
    self.len = self.x.shape[0]

  def __getitem__(self,index):
    return self.x[index],self.y[index]

  def __len__(self):
    return self.len

class Net(nn.Module):
  def __init__(self,D_in,H,D_out):
    super(Net,self).__init__()
    self.linear1 = nn.Linear(D_in,H)
    self.linear2 = nn.Linear(H,D_out)

  def forward(self,x):
    x = torch.sigmoid(self.linear1(x))
    x = torch.sigmoid(self.linear2(x))
    return x

def train(data_set,model,criterion,train_loader,optimizer,epochs=5,plot_number=10):
  cost = []
  for epoch in range(epochs):
    total = 0
    for x,y in train_loader:
      optimizer.zero_grad()
      yhat = model(x)
      loss = criterion(yhat,y)
      optimizer.zero_grad
      loss.backward()
      optimizer.step()
      total +=loss.item()
    if epoch % plot_number==0:
      PlotStuff(data_set.x,data_set.y,model)
    cost.append(total)

  plt.figure()
  plt.plot(cost)
  plt.xlabel('epoch')
  plt.ylabel('cost')
  plt.show()
  return cost

data_set=Data()
PlotStuff(data_set.x,data_set.y,leg=False)
torch.manual_seed(0)
model=Net(1,9,1)
learning_rate=0.1
criterion=nn.BCELoss()
optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)
train_loader=DataLoader(dataset=data_set,batch_size=100)
COST=train(data_set,model,criterion, train_loader, optimizer, epochs=600,plot_number=200)

"""# Practice: Neural Networks with One Hidden Layer: Noisy XOR"""

import numpy as np
import torch.nn as nn
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from torch.utils.data import Dataset,DataLoader
'''
這個函數會繪製：

模型的分類邊界（背景顏色標記決策區域）。
數據集中的點（使用不同顏色和標記表示不同類別）。
cmap_light：背景顏色，用於區分決策區域。
cmap_bold：數據點顏色，用於標記真實標籤。



'''
def plot_decision_regions_2class(model,data_set):
  cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#00AAFF'])
  cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])
  X = data_set.x.numpy()
  y = data_set.y.numpy()
  h = .02
  x_min,x_max = X[:,0].min() -0.1 , X[:,0].max() +0.1
  y_min,y_max = y[:,0].min() -0.1, y[:,0].max() + 0.1
  xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))
  '''
  生成一個 2D 網格點矩陣，讓每個點都對應一個 [x, y] 的座標。
  xx：每個網格點的 x 坐標。
  yy：每個網格點的 y 坐標。
  x_min=-1, x_max=1, h=0.5
  xx:
  [[-1.  -0.5  0.   0.5]
  [-1.  -0.5  0.   0.5]
  [-1.  -0.5  0.   0.5]
  [-1.  -0.5  0.   0.5]]

  yy:
  [[-1.  -1.  -1.  -1. ]
  [-0.5 -0.5 -0.5 -0.5]
  [ 0.   0.   0.   0. ]
  [ 0.5  0.5  0.5  0.5]]

  '''
  XX = torch.Tensor(np.c_[xx.ravel(), yy.ravel()])
  yhat = np.logical_not((model(XX)[:, 0] > 0.5).numpy()).reshape(xx.shape)
  plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)
  plt.plot(X[y[:, 0] == 0, 0], X[y[:, 0] == 0, 1], 'o', label='y=0')
  plt.plot(X[y[:, 0] == 1, 0], X[y[:, 0] == 1, 1], 'ro', label='y=1')
  plt.title("decision region")
  plt.legend()

def accuracy(model, data_set):
    X = data_set.x  # 提取特徵
    y = data_set.y  # 提取標籤
    return np.mean(y.view(-1).numpy() == (model(X)[:, 0] > 0.5).numpy())

class Net(nn.Module):
  def __init__(self,D_in,H,D_out):
    super(Net,self).__init__()
    self.linear1 = nn.Linear(D_in,H)
    self.linear2 = nn.Linear(H,D_out)

  def forward(self,x):
    x = torch.sigmoid(self.linear1(x))
    x = torch.sigmoid(self.linear2(x))
    return x

def train(data_set,model,criterion,train_loader,optimizer,epochs=5):
  COST = []
  ACC = []
  for epoch in range(epochs):
    total = 0
    for x,y in train_loader:
      optimizer.zero_grad()
      yhat = model(x)
      loss = criterion(yhat,y)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      total+=loss.item()
    ACC.append(accuracy(model,data_set))
    COST.append(total)
  fig,ax1 = plt.subplots()
  color = 'tab:red'
  ax1.plot(COST,color=color)
  ax1.set_xlabel('epoch', color=color)
  ax1.set_ylabel('total loss', color=color)
  ax1.tick_params(axis='y', color=color)

  ax2 = ax1.twinx() # 將ax1的x軸給ax2
  color = 'tab:blue'
  ax2.set_ylabel('accuracy',color=color)
  ax2.plot(ACC,color=color)
  ax2.tick_params(axis='y',color=color)
  fig.tight_layout()
  plt.show()
  return COST

class XOR_Data(Dataset):
  '''
    資料點按 XOR 規則生成：
    [0.0, 0.0]：標籤 0
    [0.0, 1.0]：標籤 1
    [1.0, 0.0]：標籤 1
    [1.0, 1.0]：標籤 0
  '''
  def __init__(self,N_s=100):
    self.x = torch.zeros((N_s,2)) #2D
    self.y = torch.zeros((N_s,1)) # label value
    for i in range(N_s // 4): # 4象限
      self.x[i,:] = torch.Tensor([0.0, 0.0])
      self.y[i,0] = torch.Tensor([0.0])
      self.x[i + N_s // 4, :] = torch.Tensor([0.0, 1.0])
      self.y[i + N_s // 4, 0] = torch.Tensor([1.0])
      self.x[i + N_s // 2, :] = torch.Tensor([1.0, 0.0])
      self.y[i + N_s // 2, 0] = torch.Tensor([1.0])
      self.x[i + 3 * N_s // 4, :] = torch.Tensor([1.0, 1.0])
      self.y[i + 3 * N_s // 4, 0] = torch.Tensor([0.0])
      self.x = self.x + 0.01 * torch.randn((N_s, 2))

    self.len = N_s

  def __getitem__(self,index):
    return self.x[index],self.y[index]

  def __len__(self):
    return self.len

  def plot_stuff(self):
    plt.plot(self.x[self.y[:,0]==0,0].numpy(),self.x[self.y[:, 0] == 0, 1].numpy(), 'o', label="y=0")
    plt.plot(self.x[self.y[:, 0] == 1, 0].numpy(), self.x[self.y[:, 0] == 1, 1].numpy(), 'ro', label="y=1")
    plt.legend()
    plt.show()

data_set = XOR_Data()
data_set.plot_stuff()
print(data_set.x.shape)

learning_rate = 0.001
model = Net(2,3,1)
criterion = nn.BCELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
train_loader = DataLoader(dataset=data_set, batch_size=1)
LOSS12 = train(data_set, model, criterion, train_loader, optimizer, epochs=500)

plot_decision_regions_2class(model, data_set)

"""># Neural Networks with One Hidden Layer"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np

def plot_acc_loss(training_results):
  plt.subplot(2,1,1)
  plt.plot(training_results['training_loss'],'r')
  plt.ylabel('loss')
  plt.title('training loss iterations')
  plt.subplot(2,1,2)
  plt.plot(training_results['validation_accuracy'])
  plt.ylabel('accuracy')
  plt.xlabel('epochs')
  plt.show()

def print_model_parameters(model):
  count = 0
  for ele in model.state_dict():
    count += 1
    if count %2 != 0:
      print("The following are the parameters for the layer ", count // 2 + 1)
    if ele.find('bias')!=-1:
      print('the size of bias ',model.state_dict()[ele].size())
    else:
      print("The size of weights: ", model.state_dict()[ele].size())

def show_data(data_sample):
  plt.imshow(data_sample.numpy().reshape(28,28),cmap='gray')
  plt.show()

class Net(nn.Module):
  def __init__(self,D_in,H,D_out):
    super(Net,self).__init__()
    self.linear1 = nn.Linear(D_in,H)
    self.linear2 = nn.Linear(H,D_out)

  def forward(self,x):
    x = torch.sigmoid(self.linear1(x))
    x = self.linear2(x)
    return x

def train(model,criterion,train_loader,validation_loader,optimizer,epochs=100):
  i=0
  useful_stuff={'training_loss':[],'validation_accuracy':[]}
  for epoch in range(epochs):
    for i,(x,y) in enumerate(train_loader):
      optimizer.zero_grad()
      z = model(x.view(-1, 28 * 28))
      loss = criterion(z,y)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      useful_stuff['training_loss'].append(loss.data.item())

    correct = 0
    for x,y in validation_loader:
      z = model(x.view(-1,28*28))
      _,label = torch.max(z,1)
      correct += (label == y).sum().item()
    accuracy = 100*(correct/len(validation_dataset))
    useful_stuff['validation_accuracy'].append(accuracy)

  return useful_stuff

train_dataset = dsets.MNIST(root='./data',train=True,download=True,transform =transforms.ToTensor())
validation_dataset = dsets.MNIST(root='./data',download=True,transform=transforms.ToTensor())
criterion = nn.CrossEntropyLoss()
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=2000,shuffle=True)
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=5000,shuffle=False)

input_dim=28*28
hidden_dim=100
output_dim=10

model = Net(input_dim,hidden_dim,output_dim)
learning_rate = 0.01
optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)
training_results = train(model,criterion,train_loader,validation_loader,optimizer,epochs=30)

plot_acc_loss(training_results)
count = 0
for x,y in validation_dataset:
  z = model(x.reshape(-1,28*28))
  _,yhat = torch.max(z,1)
  if yhat != y:
    show_data(x)
    count+=1
  if count >= 5:
    break

"""# Activation Functions"""

import torch.nn as nn
import torch
import matplotlib.pyplot as plt
torch.manual_seed(0)

z = torch.arange(-10,10,0.1).view(-1,1)

sig = nn.Sigmoid()
yhat = sig(z)
plt.plot(z.detach().numpy(),yhat.detach().numpy())
plt.xlabel('z')
plt.ylabel('yhat')

yhat = torch.sigmoid(z)
plt.plot(z.numpy(), yhat.numpy())

plt.show()

TANH = nn.Tanh()
yhat = TANH(z)
plt.plot(z.numpy(), yhat.numpy())
plt.show()
yhat = torch.tanh(z)
plt.plot(z.numpy(), yhat.numpy())
plt.show()

RELU = nn.ReLU()
yhat = RELU(z)
plt.plot(z.numpy(), yhat.numpy())
yhat = torch.relu(z)
plt.plot(z.numpy(), yhat.numpy())
plt.show()

x = torch.arange(-2, 2, 0.1).view(-1, 1)
plt.plot(x.numpy(), torch.relu(x).numpy(), label='relu')
plt.plot(x.numpy(), torch.sigmoid(x).numpy(), label='sigmoid')
plt.plot(x.numpy(), torch.tanh(x).numpy(), label='tanh')
plt.legend()

"""# Test Sigmoid, Tanh, and Relu Activations Functions on the MNIST Dataset"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets  as dsets
import matplotlib.pyplot as plt
import numpy as np

class Net(nn.Module):
  def __init__(self, D_in, H, D_out):
    super(Net, self).__init__()
    self.linear1 = nn.Linear(D_in,H)
    self.linear2 = nn.Linear(H,D_out)

  def forward(self,x):
    x = torch.sigmoid(self.linear1(x))
    x = self.linear2(x)
    return x

class NetTanh(nn.Module):
  def __init__(self, D_in, H, D_out):
    super(NetTanh, self).__init__()
    self.linear1 = nn.Linear(D_in,H)
    self.linear2 = nn.Linear(H,D_out)

  def forward(self,x):
    x = torch.tanh(self.linear1(x))
    x = self.linear2(x)
    return x


class NetRelu(nn.Module):
  def __init__(self, D_in, H, D_out):
    super(NetRelu, self).__init__()
    self.linear1 = nn.Linear(D_in, H)
    self.linear2 = nn.Linear(H, D_out)

  def forward(self, x):
    x = torch.relu(self.linear1(x))
    x = self.linear2(x)
    return x

"""# Deep Neural Networks"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
torch.manual_seed(2)

class Net(nn.Module):
  def __init__(self,D_in,H1,H2,D_out):
    super(Net,self).__init__()
    self.linear1 = nn.Linear(D_in,H1)
    self.linear2 = nn.Linear(H1,H2)
    self.linear3 = nn.Linear(H2,D_out)

  def forward(self,x):
    x = torch.sigmoid(self.linear1(x))
    x = torch.sigmoid(self.linear2(x))
    x = self.linear3(x)
    return x

class NetTanh(nn.Module):
  def __init__(self,D_in,H1,H2,D_out):
    super(NetTanh,self).__init__()
    self.linear1 = nn.Linear(D_in,H1)
    self.linear2 = nn.Linear(H1,H2)
    self.linear3 = nn.Linear(H2,D_out)

  def forward(self,x):
    x = torch.tanh(self.linear1(x))
    x = torch.tanh(self.linear2(x))
    x = self.linear3(x)
    return x

class NetRelu(nn.Module):
  def __init__(self,D_in,H1,H2,D_out):
    super(NetRelu,self).__init__()
    self.linear1 = nn.Linear(D_in,H1)
    self.linear2 = nn.Linear(H1,H2)
    self.linear3 = nn.Linear(H2,D_out)

  def forward(self,x):
    x = torch.relu(self.linear1(x))
    x = torch.relu(self.linear2(x))
    x = self.linear3(x)
    return x

def train(model,criterion,train_loader,validation_loader,optimizer,epochs=100):
  i = 0
  useful_stuff = {'training_loss':[],'validation_accuracy':[]}
  for epoch in range(epochs):
    for i,(x,y) in enumerate(train_loader):
      optimizer.zero_grad()
      z = model(x.view(-1,28*28))
      loss = criterion(z,y)
      loss.backward()
      optimizer.step()
      useful_stuff['training_loss'].append(loss.data.item())
    correct = 0
    for x,y in validation_loader:
      z = model(x.view(-1,28*28))
      _,label = torch.max(z,1)
      correct+=(label==y).sum().item()
    accuracy=100*(correct/len(validation_dataset))
    useful_stuff['validation_accuracy'].append(accuracy)

  return useful_stuff

train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())
validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())

criterion = nn.CrossEntropyLoss()
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)

input_dim = 28 * 28
hidden_dim1 = 50
hidden_dim2 = 50
output_dim = 10

cust_epochs = 10
learning_rate = 0.01
model = Net(input_dim, hidden_dim1, hidden_dim2, output_dim)
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
training_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=cust_epochs)

learning_rate = 0.01
model_Tanh = NetTanh(input_dim, hidden_dim1, hidden_dim2, output_dim)
optimizer = torch.optim.SGD(model_Tanh.parameters(), lr=learning_rate)
training_results_tanch = train(model_Tanh, criterion, train_loader, validation_loader, optimizer, epochs=cust_epochs)

learning_rate = 0.01
modelRelu = NetRelu(input_dim, hidden_dim1, hidden_dim2, output_dim)
optimizer = torch.optim.SGD(modelRelu.parameters(), lr=learning_rate)
training_results_relu = train(modelRelu, criterion, train_loader, validation_loader, optimizer, epochs=cust_epochs)

plt.plot(training_results_tanch['training_loss'], label='tanh')
plt.plot(training_results['training_loss'], label='sigmoid')
plt.plot(training_results_relu['training_loss'], label='relu')
plt.ylabel('loss')
plt.title('training loss iterations')
plt.legend()

plt.plot(training_results_tanch['validation_accuracy'], label = 'tanh')
plt.plot(training_results['validation_accuracy'], label = 'sigmoid')
plt.plot(training_results_relu['validation_accuracy'], label = 'relu')
plt.ylabel('validation accuracy')
plt.xlabel('Iteration')
plt.legend()

"""# Deeper Neural Networks with nn.ModuleList()"""

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
import torch.nn.functional as F
from matplotlib.colors import ListedColormap
from torch.utils.data import Dataset, DataLoader

def plot_decision_regions_3class(model,data_set):
  cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#00AAFF'])
  cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])
  X = data_set.x.numpy()
  y = data_set.y.numpy()
  h= .02
  x_min,x_max = X[:,0].min() -0.1, X[:,0].max()+0.1
  y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1
  xx,yy = np.meshgrid(np.arange(x_min,x_max,h)),np.arange(y_min,y_max,h)
  XX = torch.Tensor(np.c_[xx.ravel(),yy.ravel()])
  _,yhat = torch.max(model(XX),1)
  yhat = yhat.numpy().reshape(xx.shape)
  plt.pcolormesh(xx,yy,yhat,cmap=cmap_light)
  plt.plot(X[y[:]==0,0],X[y[:]==0,1],'ro',label='y=0')
  plt.plot(X[y[:]==1,0],X[y[:]==1,1],'go',label='y=1')
  plt.plot(X[y[:]==2,0],X[y[:]==2,1],'o',label='y=2')
  plt.title("decision region")
  plt.legend()
  plt.show()

class Data(Dataset):
  def __init__(self,K=3,N=500):
    D=2
    X = np.zeros((N * K, D))
    y = np.zeros(N * K, dtype='uint8')
    for j in range(K):
      ix = range(N * j, N * (j + 1))
      r = np.linspace(0.0, 1, N) # radius
      t = np.linspace(j * 4, (j + 1) * 4, N) + np.random.randn(N) * 0.2 # theta
      X[ix] = np.c_[r * np.sin(t), r*np.cos(t)]
      y[ix] = j
    self.y = torch.from_numpy(y).type(torch.LongTensor)
    self.x = torch.from_numpy(X).type(torch.FloatTensor)
    self.len = y.shape[0]

  def __getitem__(self,index):
    return self.x[index],self.y[index]

  def __len__(self):
    return self.len

  def plot_stuff(self):
    plt.plot(self.x[self.y[:] == 0, 0].numpy(), self.x[self.y[:] == 0, 1].numpy(), 'o', label="y = 0")
    plt.plot(self.x[self.y[:] == 1, 0].numpy(), self.x[self.y[:] == 1, 1].numpy(), 'ro', label="y = 1")
    plt.plot(self.x[self.y[:] == 2, 0].numpy(), self.x[self.y[:] == 2, 1].numpy(), 'go', label="y = 2")
    plt.legend()
    plt.show()

class Net(nn.Module):
  def __init__(self,Layers):
    super(Net,self).__init__()
    self.hidden=nn.ModuleList()
    for input_size,output_size in zip(Layers,Layers[1:]):
      self.hidden.append(nn.Linear(input_size,output_size))
  def forward(self,activation):
    L = len(self.hidden)
    for (l,linear_transform) in zip(range(L),self.hidden):
      if l<L-1:
        activation = F.relu(linear_transform(activation))
      else:
        activation = linear_transform(activation)
    return activation

def train(data_set,model,criterion,train_loader,optimizer,epochs=100):
  LOSS = []
  ACC = []
  for epoch in range(epochs):
    for x,y in train_loader:
      optimizer.zero_grad()
      yhat = model(x)
      loss = criterion(yhat,y)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      LOSS.append(loss.data.item())
    ACC.append(accuracy(model, data_set))

  fig,ax1 = plt.subplots()
  color = 'tab:red'
  ax1.plot(LOSS,color=color)
  ax1.set_xlabel('Iteration',color=color)
  ax1.set_ylabel('total loss',color=color)
  ax1.tick_params(axis = 'y', color = color)

  ax2 = ax1.twinx()
  color = 'tab:blue'
  ax2.set_ylabel('accuracy', color = color)
  ax2.plot(ACC, color = color)
  ax2.tick_params(axis = 'y', color = color)
  fig.tight_layout()

  plt.show()
  return LOSS

def accuracy(model, data_set):
  _, yhat = torch.max(model(data_set.x), 1)
  return (yhat == data_set.y).numpy().mean()

data_set = Data()
data_set.plot_stuff()
data_set.y = data_set.y.view(-1)
Layers = [2, 50, 3]
model = Net(Layers)
learning_rate = 0.10
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
train_loader = DataLoader(dataset=data_set, batch_size=20)
criterion = nn.CrossEntropyLoss()
LOSS = train(data_set, model, criterion, train_loader, optimizer, epochs=100)

plot_decision_regions_3class(model, data_set)

"""# Using Dropout for Classification"""

import torch
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from matplotlib.colors import ListedColormap
from torch.utils.data import Dataset, DataLoader

def plot_decision_regions_3class(data_set,model=None):
  cmap_light = ListedColormap([ '#0000FF','#FF0000'])
  cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])

  X = data_set.x.numpy()
  y = data_set.y.numpy()
  h = 0.02
  x_min , x_max = X[:,0].min()-0.1,X[:,0].max()+0.1
  y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1
  xx,yy = np.meshgrid(np.arange(x_min,x_max,h),np.arange(y_min,y_max))
  newdata = np.c_[xx.ravel(),yy.ravel()]

  Z = data_set.multi_dim_poly(newdata).flatten()
  f = np.zeros(Z.shape)
  f[Z > 0] = 1
  f = f.reshape(xx.shape)
  if model != None:
    model.eval()
    XX = torch.Tensor(newdata)
    _, yhat = torch.max(model(XX), 1)
    yhat = yhat.numpy().reshape(xx.shape)
    plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)
    plt.contour(xx, yy, f, cmap=plt.cm.Paired)
  else:
    plt.contour(xx, yy, f, cmap=plt.cm.Paired)
    plt.pcolormesh(xx, yy, f, cmap=cmap_light)
  plt.title("decision region vs True decision boundary")

def accuracy(model,data_set):
  _,yhat = torch.max(model(data_set.x),1)
  return (yhat==data_set.y).numpy().mean()

class Data(Dataset):
  def __init__(self,N_SAMPLES=1000,noise_std=0.15,train=True):
    a = np.matrix([-1, 1, 2, 1, 1, -3, 1]).T
    self.x = np.matrix(np.random.rand(N_SAMPLES,2))
    self.f = np.array(a[0] + (self.x) * a[1:3] + np.multiply(self.x[:, 0], self.x[:, 1]) * a[4] + np.multiply(self.x, self.x) * a[5:7]).flatten()
    self.a = a
    self.y = np.zeros(N_SAMPLES)
    self.y[self.f > 0] = 1
    self.y = torch.from_numpy(self.y).type(torch.LongTensor)
    self.x = torch.from_numpy(self.x).type(torch.FloatTensor)
    self.x = self.x + noise_std * torch.randn(self.x.size())
    self.f = torch.from_numpy(self.f)
    self.a = a
    if train == True:
      torch.manual_seed(1)
      self.x = self.x + noise_std * torch.randn(self.x.size())
      torch.manual_seed(0)

  def __getitem__(self,index):
    return self.x[index],self.y[index]

  def __len__(self):
    return self.len

  def plot(self):
    X = data_set.x.numpy()
    y = data_set.y.numpy()
    h = 0.02
    x_min , x_max = X[:,0].min()-0.1,X[:,0].max()+0.1
    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

    Z = data_set.multi_dim_poly(np.c_[xx.ravel(), yy.ravel()]).flatten()
    f = np.zeros(Z.shape)
    f[Z > 0] = 1
    f = f.reshape(xx.shape)
    plt.title('True decision boundary  and sample points with noise ')
    plt.plot(self.x[self.y == 0, 0].numpy(), self.x[self.y == 0,1].numpy(), 'bo', label='y=0')
    plt.plot(self.x[self.y == 1, 0].numpy(), self.x[self.y == 1,1].numpy(), 'ro', label='y=1')
    plt.contour(xx, yy, f,cmap=plt.cm.Paired)
    plt.xlim(0,1)
    plt.ylim(0,1)
    plt.legend()

  def multi_dim_poly(self, x):
    x = np.matrix(x)
    out = np.array(self.a[0] + (x) * self.a[1:3] + np.multiply(x[:, 0], x[:, 1]) * self.a[4] + np.multiply(x, x) * self.a[5:7])
    out = np.array(out)
    return out

data_set = Data(noise_std=0.2)
data_set.plot()

torch.manual_seed(0)
validation_set = Data(train=False)

class Net(nn.Module):
  def __init__(self,in_size,n_hidden,out_size,p=0):
    super(Net,self).__init__()
    self.drop = nn.Dropout(p=p)
    self.linear1 = nn.Linear(in_size,n_hidden)
    self.linear2 = nn.Linear(n_hidden,n_hidden)
    self.linear3 = nn.Linear(n_hidden,out_size)

  def forward(self,x):
    x = F.relu(self.drop(self.linear1(x)))
    x = F.relu(self.drop(self.linear2(x)))
    x = self.linear3(x)
    return x

model = Net(2, 300, 2)
model_drop = Net(2, 300, 2, p=0.5)

model_drop.train()
optimizer_ofit = torch.optim.Adam(model.parameters(), lr=0.01)
optimizer_drop = torch.optim.Adam(model_drop.parameters(), lr=0.01)
criterion = torch.nn.CrossEntropyLoss()

LOSS = {}
LOSS['training data no dropout'] = []
LOSS['validation data no dropout'] = []
LOSS['training data dropout'] = []
LOSS['validation data dropout'] = []

epochs = 500
def train_model(epochs):
  for epoch in range(epochs):
    yhat = model(data_set.x)
    yhat_drop = model_drop(data_set.x)
    loss = criterion(yhat, data_set.y)
    loss_drop = criterion(yhat_drop, data_set.y)

    LOSS['training data no dropout'].append(loss.item())
    LOSS['validation data no dropout'].append(criterion(model(validation_set.x), validation_set.y).item())
    LOSS['training data dropout'].append(loss_drop.item())
    model_drop.eval()
    LOSS['validation data dropout'].append(criterion(model_drop(validation_set.x), validation_set.y).item())
    model_drop.train()

    optimizer_ofit.zero_grad()
    optimizer_drop.zero_grad()
    loss.backward()
    loss_drop.backward()
    optimizer_ofit.step()
    optimizer_drop.step()

train_model(epochs)

model_drop.eval()
plot_decision_regions_3class(data_set)
plot_decision_regions_3class(data_set, model)
plot_decision_regions_3class(data_set, model_drop)

"""# Using Dropout in Regression"""

import torch
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.utils.data import Dataset, DataLoader

torch.manual_seed(0)


class Data(Dataset):
  def __init__(self,N_SAMPLES=40,noise_std=1,train=True):
    self.x = torch.linspace(-1,1,N_SAMPLES).view(-1,1)
    self.f = self.x ** 2
    if train != True:
      torch.manual_seed(1)
      self.y = self.f + noise_std * torch.randn(self.f.size())
      self.y = self.y.view(-1,1)
      torch.manual_seed(0)
    else:
      self.y = self.f+noise_std*torch.randn(self.f.size())
      self.y = self.y.view(-1,1)

  def __getitem__(self,index):
    return self.x[index],self.y[index]

  def __len__(self):
    return self.len

  def plot(self):
    plt.figure(figsize=(6.1,10))
    plt.scatter(self.x.numpy(),self.y.numpy(), label="Samples")
    plt.plot(self.x.numpy(),self.f.numpy(),label='True Function',color='orange')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.xlim((-1, 1))
    plt.ylim((-2, 2.5))
    plt.legend(loc="best")
    plt.show()

data_set = Data()
data_set.plot()
validation_set = Data(train=False)

class Net(nn.Module):
  def __init__(self,in_size,n_hidden,out_size,p=0):
    super(Net,self).__init__()
    self.drop = nn.Dropout(p = p)
    self.linear1 = nn.Linear(in_size,n_hidden)
    self.linear2 = nn.Linear(n_hidden,n_hidden)
    self.linear3 = nn.Linear(n_hidden,out_size)

  def forward(self,x):
    x = F.relu(self.drop(self.linear1(x)))
    x = F.relu(self.drop(self.linear2(x)))
    x = self.linear3(x)
    return x

model = Net(1, 300, 1)
model_drop = Net(1, 300, 1, p=0.5)

model_drop.train()

optimizer_ofit = torch.optim.Adam(model.parameters(), lr=0.01)
optimizer_drop = torch.optim.Adam(model_drop.parameters(), lr=0.01)
criterion = torch.nn.MSELoss()
LOSS={}
LOSS['training data no dropout']=[]
LOSS['validation data no dropout']=[]
LOSS['training data dropout']=[]
LOSS['validation data dropout']=[]

epochs = 500
def train_model(epochs):
  for epoch in range(epochs):
    yhat = model(data_set.x)
    yhat_drop = model_drop(data_set.x)
    loss = criterion(yhat,data_set.y)
    loss_drop = criterion(yhat_drop,data_set.y)
    LOSS['training data no dropout'].append(loss.item())
    LOSS['validation data no dropout'].append(criterion(model(validation_set.x), validation_set.y).item())
    LOSS['training data dropout'].append(loss_drop.item())
    model_drop.eval()
    LOSS['validation data dropout'].append(criterion(model_drop(validation_set.x), validation_set.y).item())
    model_drop.train()

    optimizer_ofit.zero_grad()
    optimizer_drop.zero_grad()
    loss.backward()
    loss_drop.backward()
    optimizer_ofit.step()
    optimizer_drop.step()

train_model(epochs)

model_drop.eval()
yhat = model(data_set.x)
yhat_drop = model_drop(data_set.x)
plt.figure(figsize=(6.1, 10))

plt.scatter(data_set.x.numpy(), data_set.y.numpy(), label="Samples")
plt.plot(data_set.x.numpy(), data_set.f.numpy(), label="True function", color='orange')
plt.plot(data_set.x.numpy(), yhat.detach().numpy(), label='no dropout', c='r')
plt.plot(data_set.x.numpy(), yhat_drop.detach().numpy(), label="dropout", c ='g')

plt.xlabel("x")
plt.ylabel("y")
plt.xlim((-1, 1))
plt.ylim((-2, 2.5))
plt.legend(loc = "best")
plt.show()
plt.figure(figsize=(6.1, 10))
for key, value in LOSS.items():
    plt.plot(np.log(np.array(value)), label=key)
    plt.legend()
    plt.xlabel("iterations")
    plt.ylabel("Log of cost or total loss")

"""# Initialization with Same Weights"""

import torch
import torch.nn as nn
from torch import sigmoid
import matplotlib.pyplot as plt
import numpy as np
torch.manual_seed(0)

def PlotStuff(X,Y,model,epoch,leg=True):
  plt.plot(X.numpy(),model(X).detach().numpy(),label=('epoch'+str(epoch)))
  plt.plot(X.numpy(),Y.numpy(),'r')
  plt.xlabel('x')
  if leg == True:
    plt.legend()
  else:
    pass

class Net(nn.Module):
  def __init__(self,D_in,H,D_out):
    super(Net,self).__init__()
    self.linear1 = nn.Linear(D_in,H)
    self.linear2 = nn.Linear(H,D_out)
    self.a1 = None
    self.l1 = None
    self.l2 = None

  def forward(self,x):
    self.l1 = self.linear1(x)
    self.a1 = sigmoid(self.l1)
    self.l2 = self.linear2(self.a1)
    yhat = sigmoid(self.l2)
    return yhat

def train(Y,X,model,optimizer,criterion,epochs=1000):
  cost = []
  total = 0
  for epoch in range(epochs):
    total = 0
    for y,x in zip(Y,X):
      yhat = model(x)
      loss = criterion(yhat,y)
      #optimizer.zero_grad() 因為初始化
      loss.backward()
      optimizer.step()
      optimizer.zero_grad()
      total += loss.item()
    cost.append(total)
    if epoch % 300 == 0:
      PlotStuff(X, Y, model, epoch, leg=True)
      plt.show()
      model(X)
      print(model.a1.detach())
      print(model.a1.detach().numpy())
      plt.scatter(model.a1.detach().numpy()[:, 0], model.a1.detach().numpy()[:, 1], c=Y.numpy().reshape(-1))
      plt.title('activations')
      plt.show()
  return cost

X = torch.arange(-20,20,1).view(-1,1).type(torch.FloatTensor)
Y = torch.zeros(X.shape[0])
Y[(X[:, 0] > -4) & (X[:, 0] < 4)] = 1.0

def criterion_cross(outputs, labels): # bce loss binary cross entropy
  out = -1 * torch.mean(labels * torch.log(outputs) + (1 - labels) * torch.log(1 - outputs))
  return out

D_in = 1
H = 2
D_out = 1
learning_rate = 0.1
model = Net(D_in, H, D_out)
print(model.state_dict())
# initialize weight
model.state_dict()['linear1.weight'][0]=1.0
model.state_dict()['linear1.weight'][1]=1.0
model.state_dict()['linear1.bias'][0]=0.0
model.state_dict()['linear1.bias'][1]=0.0
model.state_dict()['linear2.weight'][0]=1.0
model.state_dict()['linear2.bias'][0]=0.0
model.state_dict()

optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

cost_cross = train(Y, X, model, optimizer, criterion_cross, epochs=1000)
plt.plot(cost_cross)
plt.xlabel('epoch')
plt.title('cross entropy loss')

yhat=model(torch.tensor([[-2.0],[0.0],[2.0]]))
yhat

"""# Test Uniform, Default and Xavier Uniform Initialization on MNIST dataset with tanh activation"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import matplotlib.pylab as plt
import numpy as np

torch.manual_seed(0)

class Net_Xaiver(nn.Module):
  def __init__(self,Layers):
    super(Net_Xaiver,self).__init__()
    self.hidden = nn.ModuleList()

    for input_size,output_size in zip(Layers,Layers[1:]):
      linear = nn.Linear(input_size,output_size)
      torch.nn.init.xavier_uniform_(linear.weight)
      self.hidden.append(linear)

  def forward(self,x):
    L = len(self.hidden)
    for (l,linear_transform) in zip(range(L),self.hidden):
      if l < L-1:
        x = torch.tanh(linear_transform(x)) # 運用tanh函數 (而非sigmoid)
      else:
        x = linear_transform(x)
    return x

class Net_Uniform(nn.Module):
  def __init__(self,Layers):
    super(Net_Uniform,self).__init__()
    self.hidden = nn.ModuleList()
    for input_size,output_size in zip(Layers,Layers[1:]):
      linear = nn.Linear(input_size,output_size)
      linear.weight.data.uniform_(0,1) #用常態分佈去初始化向量
      self.hidden.append(linear)
  def forward(self,x):
    L = len(self.hidden)
    for (l,linear_transform) in zip(range(L),self.hidden):
      if l < L-1:
        x = torch.tanh(linear_transform(x))
      else:
        x = linear_transform(x)
    return x

class Net(nn.Module):
  def __init__(self,Layers):
    super(Net,self).__init__()
    self.hidden = nn.ModuleList()
    for input_size,output_size in zip(Layers,Layers[1:]):
      linear = nn.Linear(input_size,output_size)
      self.hidden.append(linear)

  def forward(self,x):
    L = len(self.hidden)
    for(l,linear_transform) in zip(range(L),self.hidden):
      if l < L-1:
        x = torch.tanh(linear_transform(x))
      else:
        x = linear_transform(x)
    return x

def train(model,criterion,train_loader,validation_loader,optimizer,epochs=100):
  i = 0
  loss_acc = {'training_loss':[], 'validation_accuracy':[]}
  for epoch in range(epochs):
    for i,(x,y) in enumerate(train_loader):
      optimizer.zero_grad()
      z = model(x.view(-1,28*28))
      loss = criterion(z,y)
      loss.backward()
      optimizer.step()
      optimizer.zero_grad()
      loss_acc['training_loss'].append(loss.data.item())
    correct = 0
    for x,y in validation_loader:
      yhat = model(x.view(-1,28*28))
      _,label = torch.max(yhat,1)
      correct += (label==y).sum().item()
    accuracy = 100 * (correct / len(validation_dataset))
    loss_acc['validation_accuracy'].append(accuracy)
  return loss_acc

train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())
validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)

criterion = nn.CrossEntropyLoss()
input_dim = 28 * 28
output_dim = 10
layers = [input_dim, 100, 10, 100, 10, 100, output_dim]
epochs = 15

model = Net(layers)
learning_rate = 0.01
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
training_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=epochs)

model_Xavier = Net_Xaiver(layers)
optimizer = torch.optim.SGD(model_Xavier.parameters(), lr=learning_rate)
training_results_Xavier = train(model_Xavier, criterion, train_loader, validation_loader, optimizer, epochs=epochs)

model_Uniform = Net_Uniform(layers)
optimizer = torch.optim.SGD(model_Uniform.parameters(), lr=learning_rate)
training_results_Uniform = train(model_Uniform, criterion, train_loader, validation_loader, optimizer, epochs=epochs)

plt.plot(training_results_Xavier['training_loss'], label='Xavier')
plt.plot(training_results['training_loss'], label='Default')
plt.plot(training_results_Uniform['training_loss'], label='Uniform')
plt.ylabel('loss')
plt.xlabel('iteration ')
plt.title('training loss iterations')
plt.legend()

plt.plot(training_results_Xavier['validation_accuracy'], label='Xavier')
plt.plot(training_results['validation_accuracy'], label='Default')
plt.plot(training_results_Uniform['validation_accuracy'], label='Uniform')
plt.ylabel('validation accuracy')
plt.xlabel('epochs')
plt.legend()

"""# Test Uniform, Default and He Initialization on MNIST Dataset with Relu Activation"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import torch.nn.functional as F
import matplotlib.pylab as plt
import numpy as np

torch.manual_seed(0)

class Net_He(nn.Module):
  def __init__(self,Layers):
    super(Net_He,self).__init__()
    self.hidden = nn.ModuleList()
    for input_size,output_size in zip(Layers,Layers[1:]):
      linear = nn.Linear(input_size,output_size)
      torch.nn.init.kaiming_uniform_(linear.weight, nonlinearity='relu')
      self.hidden.append(linear)

  def forward(self,x):
    L = len(self.hidden)
    for(l,linear_transform) in zip(range(L),self.hidden):
      if l < L-1:
        x = F.relu(linear_transform(x))
      else:
        x = linear_transform(x)
    return x

class Net_Uniform(nn.Module):
  def __init__(self,Layers):
    super(Net_Uniform,self).__init__()
    self.hidden = nn.ModuleList()

    for input_size,output_size in zip(Layers,Layers[1:]):
      linear = nn.Linear(input_size,output_size)
      linear.weight.data.uniform_(0,1)
      self.hidden.append(linear)

  def forward(self,x):
    L = len(self.hidden)
    for (l,linear_transform) in zip(range(L),self.hidden):
      if l<L-1:
        x = F.relu(linear_transform(x))
      else:
        x = linear_transform(x)
    return x

class Net(nn.Module):
  def __init__(self,Layers):
    super(Net,self).__init__()
    self.hidden = nn.ModuleList()

    for input_size,output_size in zip(Layers,Layers[1:]):
      linear = nn.Linear(input_size,output_size)
      self.hidden.append(linear)

  def forward(self,x):
    L = len(self.hidden)
    for (l,linear_transform) in zip(range(L),self.hidden):
      if l < L - 1:
        x = F.relu(linear_transform(x))
      else:
        x = linear_transform(x)
    return x

model = Net(layers)
learning_rate = 0.01
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
training_results = train(model, criterion, train_loader,validation_loader, optimizer, epochs=30)

model_He = Net_He(layers)
optimizer = torch.optim.SGD(model_He.parameters(), lr=learning_rate)
training_results_He = train(model_He, criterion, train_loader, validation_loader, optimizer, epochs=30)

model_Uniform = Net_Uniform(layers)
optimizer = torch.optim.SGD(model_Uniform.parameters(), lr=learning_rate)
training_results_Uniform = train(model_Uniform, criterion, train_loader, validation_loader, optimizer, epochs=30)

plt.plot(training_results_He['training_loss'], label='He')
plt.plot(training_results['training_loss'], label='Default')
plt.plot(training_results_Uniform['training_loss'], label='Uniform')
plt.ylabel('loss')
plt.xlabel('iteration ')
plt.title('training loss iterations')
plt.legend()

plt.plot(training_results_He['validation_accuracy'], label='He')
plt.plot(training_results['validation_accuracy'], label='Default')
plt.plot(training_results_Uniform['validation_accuracy'], label='Uniform')
plt.ylabel('validation accuracy')
plt.xlabel('epochs ')
plt.legend()
plt.show()

"""# Neural Networks with Momentum"""

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from matplotlib.colors import ListedColormap
from torch.utils.data import Dataset, DataLoader

torch.manual_seed(1)
np.random.seed(1)

def plot_decision_regions_3class(model,data_set):
  cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA','#00AAFF'])
  cmap_bold = ListedColormap(['#FF0000', '#00FF00','#00AAFF'])
  X = data_set.x.numpy()
  y = data_set.y.numpy()
  h = 0.02
  x_min, x_max = X[:, 0].min() - 0.1 , X[:, 0].max() + 0.1
  y_min, y_max = X[:, 1].min() - 0.1 , X[:, 1].max() + 0.1
  xx,yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))
  XX=torch.torch.Tensor(np.c_[xx.ravel(), yy.ravel()])
  _,yhat=torch.max(model(XX),1)
  yhat=yhat.numpy().reshape(xx.shape)
  plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)
  plt.plot(X[y[:]==0,0], X[y[:]==0,1], 'ro', label='y=0')
  plt.plot(X[y[:]==1,0], X[y[:]==1,1], 'go', label='y=1')
  plt.plot(X[y[:]==2,0], X[y[:]==2,1], 'o', label='y=2')
  plt.title("decision region")
  plt.legend()
  plt.show()


class Data(Dataset):
  def __init__(self,K=3,N=500):
    D = 2
    X = np.zeros((N * K, D)) # data matrix (each row = single example)
    y = np.zeros(N * K, dtype='uint8') # class labels
    for j in range(K):
      ix = range(N * j, N * (j + 1))
      r = np.linspace(0.0,1,N)
      t = np.linspace(j * 4, (j + 1) * 4, N) + np.random.randn(N) * 0.2
      X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]
      y[ix] = j

    self.y = torch.from_numpy(y).type(torch.LongTensor)
    self.x = torch.from_numpy(X).type(torch.FloatTensor)
    self.len = y.shape[0]

  def __getitem__(self, index):
    return self.x[index], self.y[index]

    # Get Length
  def __len__(self):
    return self.len

    # Plot the diagram
  def plot_data(self):
    plt.plot(self.x[self.y[:] == 0, 0].numpy(), self.x[self.y[:] == 0, 1].numpy(), 'o', label="y=0")
    plt.plot(self.x[self.y[:] == 1, 0].numpy(), self.x[self.y[:] == 1, 1].numpy(), 'ro', label="y=1")
    plt.plot(self.x[self.y[:] == 2, 0].numpy(),self.x[self.y[:] == 2, 1].numpy(), 'go',label="y=2")
    plt.legend()


class Net(nn.Module):
  def __init__(self,Layers):
    super(Net,self).__init__()
    self.hidden = nn.ModuleList()
    for input_size,output_size in zip(Layers,Layers[1:]):
      linear = nn.Linear(input_size,output_size)
      self.hidden.append(linear)

  def forward(self,x):
    L = len(self.hidden)
    for(l,linear_transform) in zip(range(L),self.hidden):
      if l < L-1:
        x = F.relu(linear_transform(x))
      else:
        x = linear_transform(x)
    return x

def accuracy(model, data_set):
    _, yhat = torch.max(model(data_set.x), 1)
    return (yhat == data_set.y).numpy().mean()

def train(data_set,model,criterion,train_loader,optimizer,epochs=100):
  LOSS = []
  ACC = []
  for epoch in range(epochs):
    for x,y in train_loader:
      optimizer.zero_grad()
      yhat = model(x)
      loss = criterion(yhat,y)
      loss.backward()
      optimizer.step()
      optimizer.zero_grad()
    LOSS.append(loss.item())
    ACC.append(accuracy(model,data_set))

  results ={"Loss":LOSS, "Accuracy":ACC}
  fig,ax1 = plt.subplots()
  color = 'tab:red'
  ax1.plot(LOSS,color=color)
  ax1.set_xlabel('epoch', color=color)
  ax1.set_ylabel('total loss', color=color)
  ax1.tick_params(axis = 'y', color=color)

  ax2 = ax1.twinx()
  color = 'tab:blue'
  ax2.set_ylabel('accuracy', color=color)
  ax2.plot(ACC, color=color)
  ax2.tick_params(axis='y', color=color)
  fig.tight_layout()
  plt.show()
  return results

data_set = Data()
data_set.plot_data()
data_set.y = data_set.y.view(-1)
Results = {"momentum 0": {"Loss": 0, "Accuracy:": 0}, "momentum 0.1": {"Loss": 0, "Accuracy:": 0}}
Layers = [2, 50, 3]
model = Net(Layers)
learning_rate = 0.10
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
train_loader = DataLoader(dataset=data_set, batch_size=20)
criterion = nn.CrossEntropyLoss()

Results["momentum 0"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100)
plot_decision_regions_3class(model, data_set)
Layers = [2, 50, 3]
model = Net(Layers)
learning_rate = 0.10
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.1) ####
train_loader = DataLoader(dataset=data_set, batch_size=20)
criterion = nn.CrossEntropyLoss()
Results["momentum 0.1"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100)
plot_decision_regions_3class(model, data_set)


Layers = [2, 50, 3]
model = Net(Layers)
learning_rate = 0.10
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.2) #####
train_loader = DataLoader(dataset=data_set, batch_size=20)
criterion = nn.CrossEntropyLoss()
Results["momentum 0.2"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100)
plot_decision_regions_3class(model, data_set)

Layers = [2, 50, 3]
model = Net(Layers)
learning_rate = 0.10
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.4)
train_loader = DataLoader(dataset=data_set, batch_size=20)
criterion = nn.CrossEntropyLoss()
Results["momentum 0.4"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100)
plot_decision_regions_3class(model, data_set)

Layers = [2, 50, 3]
model = Net(Layers)
learning_rate = 0.10
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.5)
train_loader = DataLoader(dataset=data_set, batch_size=20)
criterion = nn.CrossEntropyLoss()
Results["momentum 0.5"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100)
plot_decision_regions_3class(model,data_set)

for key, value in Results.items():
    plt.plot(value['Loss'],label=key)
    plt.legend()
    plt.xlabel('epoch')
    plt.ylabel('Total Loss or Cost')


for key, value in Results.items():
    plt.plot(value['Accuracy'],label=key)
    plt.legend()
    plt.xlabel('epoch')
    plt.ylabel('Accuracy')



"""# Batch Normalization with the MNIST Dataset"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import torch.nn.functional as F
import matplotlib.pylab as plt
import numpy as np
torch.manual_seed(0)

class NetBatchNorm(nn.Module):
  def __init__(self,in_size,n_hidden1,n_hidden2,out_size):
    super(NetBatchNorm,self).__init__()
    self.linear1 = nn.Linear(in_size,n_hidden1)
    self.linear2 = nn.Linear(n_hidden1,n_hidden2)
    self.linear3 = nn.Linear(n_hidden2,out_size)
    self.bn1 = nn.BatchNorm1d(n_hidden1)
    self.bn2 = nn.BatchNorm1d(n_hidden2)

  def forward(self,x):
    x = self.bn1(torch.sigmoid(self.linear1(x)))
    x = self.bn2(torch.sigmoid(self.linear2(x)))
    x = self.linear3(x)
    return x

  def activation(self,x):
    out = []
    z1 = self.bn1(self.linear1(x))
    out.append(z1.detach().numpy().reshape(-1))
    a1 = torch.sigmoid(z1)
    out.append(a1.detach().numpy().reshape(-1).reshape(-1))
    z2 = self.bn2(self.linear2(a1))
    out.append(z2.detach().numpy().reshape(-1))
    a2 = torch.sigmoid(z2)
    out.append(a2.detach().numpy().reshape(-1))
    return out

class Net(nn.Module):
  def __init__(self,in_size,n_hidden1,n_hidden2,out_size):
    super(Net,self).__init__()
    self.linear1 = nn.Linear(in_size,n_hidden1)
    self.linear2 = nn.Linear(n_hidden1,n_hidden2)
    self.linear3 = nn.Linear(n_hidden2,out_size)

  def forward(self,x):
    x = torch.sigmoid(self.linear1(x))
    x = torch.sigmoid(self.linear2(x))
    x = self.linear3(x)
    return x

  def activation(self,x):
    out = []
    z1 = self.linear1(x)
    out.append(z1.detach().numpy().reshape(-1))
    a1 = torch.sigmoid(z1)
    out.append(a1.detach().numpy().reshape(-1).reshape(-1))
    z2 = self.linear2(a1)
    out.append(z2.detach().numpy().reshape(-1))
    a2 = torch.sigmoid(z2)
    out.append(a2.detach().numpy().reshape(-1))
    return out

def train(model,criterion,train_loader,validation_loader,optimizer,epochs=100):
  i = 0
  useful_stuff = {'training_loss':[],'validation_accuracy':[]}

  for epoch in range(epochs):
    for i,(x,y) in enumerate(train_loader):
      model.train()
      optimizer.zero_grad()
      z = model(x.view(-1,28*28))
      loss = criterion(z,y)
      loss.backward()
      optimizer.step()
      useful_stuff['training_loss'].append(loss.data.item())
    correct = 0
    for x,y in validation_loader:
      model.eval()
      yhat = model(x.view(-1,28*28))
      _,label = torch.max(yhat,1)
      correct += (label==y).sum().item()

    accuracy = 100 * (correct/len(validation_dataset))
    useful_stuff['validation_accuracy'].append(accuracy)
  return useful_stuff

train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())
validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)
criterion = nn.CrossEntropyLoss()

input_dim = 28 * 28
hidden_dim = 100
output_dim = 10
model_norm  = NetBatchNorm(input_dim, hidden_dim, hidden_dim, output_dim)
optimizer = torch.optim.Adam(model_norm.parameters(), lr = 0.1)

training_results_Norm=train(model_norm , criterion, train_loader, validation_loader, optimizer, epochs=5)

model = Net(input_dim, hidden_dim, hidden_dim, output_dim)
optimizer = torch.optim.Adam(model.parameters(), lr = 0.1)
training_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=5)

model.eval()
model_norm.eval()
out=model.activation(validation_dataset[0][0].reshape(-1,28*28))
plt.hist(out[2],label='model with no batch normalization' )
out_norm=model_norm.activation(validation_dataset[0][0].reshape(-1,28*28))
plt.hist(out_norm[2],label='model with normalization')
plt.xlabel("activation ")
plt.legend()
plt.show()

plt.plot(training_results['training_loss'], label='No Batch Normalization')
plt.plot(training_results_Norm['training_loss'], label='Batch Normalization')
plt.ylabel('Cost')
plt.xlabel('iterations ')
plt.legend()
plt.show()

plt.plot(training_results['validation_accuracy'],label='No Batch Normalization')
plt.plot(training_results_Norm['validation_accuracy'],label='Batch Normalization')
plt.ylabel('validation accuracy')
plt.xlabel('epochs ')
plt.legend()
plt.show()



"""# Activation function and Maxpooling"""

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from scipy import ndimage, misc

conv = nn.Conv2d(in_channels=1,out_channels=1,kernel_size=3)
Gx = torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0,-1.0]])
conv.state_dict()['weight'][0][0] = Gx
conv.state_dict()['bias'][0]=0.0
image = torch.zeros(1,1,5,5)
image[0,0,:,2]=1
print(image)
Z=conv(image)
A=torch.relu(Z)
relu = nn.ReLU()
relu(Z)

image1=torch.zeros(1,1,4,4)
image1[0,0,0,:]=torch.tensor([1.0,2.0,3.0,-4.0])
image1[0,0,1,:]=torch.tensor([0.0,2.0,-3.0,0.0])
image1[0,0,2,:]=torch.tensor([0.0,2.0,3.0,1.0])
max1=torch.nn.MaxPool2d(2,stride=1)
max1(image1)

max1=torch.nn.MaxPool2d(2)
max1(image1)

"""# Multiple Input and Output Channels"""

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from scipy import ndimage, misc

conv1 = nn.Conv2d(in_channels=1,out_channels=3,kernel_size=3)
Gx = torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])
Gy=torch.tensor([[1.0,2.0,1.0],[0.0,0.0,0.0],[-1.0,-2.0,-1.0]])

conv1.state_dict()['weight'][0][0] = Gx
conv1.state_dict()['weight'][1][0] = Gy
conv1.state_dict()['weight'][2][0] = torch.ones(3,3)

conv1.state_dict()['bias'][:]=torch.tensor([0.0,0.0,0.0])
conv1.state_dict()['bias']

for x in conv1.state_dict()['weight']:
  print(x)

image = torch.zeros(1,1,5,5)
image[0,0,:,2] = 1
print(image)
plt.imshow(image[0,0,:,:].numpy(), interpolation='nearest', cmap=plt.cm.gray)
plt.colorbar()
plt.show()

out=conv1(image)
for channel,image in enumerate(out[0]):
  plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)
  print(image)
  plt.title("channel {}".format(channel))
  plt.colorbar()
  plt.show()

image1=torch.zeros(1,1,5,5)
image1[0,0,2,:]=1
print(image1)
plt.imshow(image1[0,0,:,:].detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)
plt.show()

out1=conv1(image1)
for channel,image in enumerate(out1[0]):
  plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)
  print(image)
  plt.title("channel {}".format(channel))
  plt.colorbar()
  plt.show()

image2=torch.zeros(1,2,5,5)
image2[0,0,2,:]=-2
image2[0,1,2,:]=1
image2

conv3 = nn.Conv2d(in_channels=2, out_channels=1,kernel_size=3)
Gx1=torch.tensor([[0.0,0.0,0.0],[0,1.0,0],[0.0,0.0,0.0]])
conv3.state_dict()['weight'][0][0]=1*Gx1
conv3.state_dict()['weight'][0][1]=-2*Gx1
conv3.state_dict()['bias'][:]=torch.tensor([0.0])
conv3.state_dict()['weight']

conv4 = nn.Conv2d(in_channels=2, out_channels=3,kernel_size=3)
conv4.state_dict()['weight'][0][0]=torch.tensor([[0.0,0.0,0.0],[0,0.5,0],[0.0,0.0,0.0]])
conv4.state_dict()['weight'][0][1]=torch.tensor([[0.0,0.0,0.0],[0,0.5,0],[0.0,0.0,0.0]])


conv4.state_dict()['weight'][1][0]=torch.tensor([[0.0,0.0,0.0],[0,1,0],[0.0,0.0,0.0]])
conv4.state_dict()['weight'][1][1]=torch.tensor([[0.0,0.0,0.0],[0,-1,0],[0.0,0.0,0.0]])

conv4.state_dict()['weight'][2][0]=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])
conv4.state_dict()['weight'][2][1]=torch.tensor([[1.0,2.0,1.0],[0.0,0.0,0.0],[-1.0,-2.0,-1.0]])

conv4.state_dict()['bias'][:]=torch.tensor([0.0,0.0,0.0])

image4=torch.zeros(1,2,5,5)
image4[0][0]=torch.ones(5,5)
image4[0][1][2][2]=1
for channel,image in enumerate(image4[0]):
  plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)
  print(image)
  plt.title("channel {}".format(channel))
  plt.colorbar()
  plt.show()
z=conv4(image4)
z

"""# Convolutional Neural Network Simple example"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

torch.manual_seed(4)

def plot_channels(W): # 顯示weight
  n_out = W.shape[0]
  n_in = W.shape[1]
  w_min = W.min().item()
  w_max = W.max().item()
  fig, axes = plt.subplots(n_out,n_in)
  fig.subplots_adjust(hspace = 0.1)
  out_index = 0
  in_index = 0
  for ax in axes.flat:
    if in_index > n_in-1:
      out_index = out_index + 1
      in_index = 0
    ax.imshow(W[out_index,in_index,:,:], vmin=w_min, vmax=w_max, cmap='seismic')
    ax.set_yticklabels([])
    ax.set_xticklabels([])
    in_index=in_index+1

  plt.show()

def show_data(dataset,sample):
  plt.imshow(dataset.x[sample,0,:,:].numpy(),cmap='gray')
  plt.title('y='+str(dataset.y[sample].item()))
  plt.show()

from torch.utils.data import Dataset, DataLoader
class Data(Dataset):
  def __init__(self,N_images=100,offset=0,p=0.9,train=False):
    if train==True:
      np.random.seed(1)

    N_images = 2*(N_images//2)
    images = np.zeros((N_images,1,11,11))
    start1 = 3
    start2 = 1
    self.y = torch.zeros(N_images).type(torch.long)

    for n in range(N_images):
      if offset > 0:
        low = int(np.random.randint(low=start1, high=start1+offset, size=1))
        high = int(np.random.randint(low=start2, high=start2+offset, size=1))
      else:
        low = 4
        high = 1
      if n <= N_images//2:
        self.y[n]=0
        images[n,0,high:high+9,low:low+3]= np.random.binomial(1, p, (9,3))
      elif n > N_images//2:
        self.y[n] =1
        images[n,0,low:low+3,high:high+9] = np.random.binomial(1, p, (3,9))

    self.x=torch.from_numpy(images).type(torch.FloatTensor)
    self.len=self.x.shape[0]
    del(images)
    np.random.seed(0)

  def __getitem__(self,index):
    return self.x[index], self.y[index]

  def __len__(self):
    return self.len

def plot_activations(A,number_rows=1,name=""):
  A = A[0,:,:,:].detach().numpy()
  n_activations = A.shape[0]

  print(n_activations)
  A_min = A.min().item()
  A_max = A.max().item()

  if n_activations==1:
    plt.imshow(A[0,:], vmin=A_min, vmax=A_max, cmap='seismic')
  else:
    fig,axes = plt.subplots(number_rows,n_activations // number_rows)
    fig.subplots_adjust(hspace=0.4)
    for i,ax in enumerate(axes.flat): #ax.flat 是 Matplotlib 中的屬性，用於將子圖 (Axes) 的多維結構 展平成一維迭代器。
      if i < n_activations:
        ax.set_xlabel( "activation:{0}".format(i+1))
        ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')
        ax.set_xticks([]) # 將子圖的 x 軸刻度標記設置為空（隱藏 x 軸上的刻度）。
        ax.set_yticks([])
  plt.show()

def conv_output_shape(h_w,kernel_size=1,stride=1,pad=0,dilation=1):
  from math import floor
  if type(kernel_size) is not tuple:
    kernel_size = (kernel_size,kernel_size)

  h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)
  w = floor(((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)
  return h,w

N_images = 10000
train_dataset = Data(N_images)
validation_dataset=Data(N_images=1000,train=False)

show_data(train_dataset,0)
show_data(train_dataset,N_images//2+2)

out = conv_output_shape((11,11),kernel_size=2,stride=1,pad=0,dilation=1)
print(out)

out1 = conv_output_shape(out,kernel_size=2,stride=1,pad=0,dilation=1)
print(out1)

out2 = conv_output_shape(out1,kernel_size=2,stride=1,pad=0,dilation=1)
print(out2)

out3 = conv_output_shape(out2,kernel_size=2,stride=1,pad=0,dilation=1)
print(out3)

class CNN(nn.Module):
  def __init__(self,out_1=2,out_2=1):
    super(CNN,self).__init__()
    self.cnn1 = nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)
    self.maxpool1 = nn.MaxPool2d(kernel_size=2,stride=1)

    self.cnn2 = nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)
    self.maxpool2 = nn.MaxPool2d(kernel_size=2,stride=1)

    self.fc1=nn.Linear(out_2*7*7,2) #reshape and fcn 兩個類別

  def forward(self,x):
    x = self.cnn1(x)
    x = torch.relu(x)
    x = self.maxpool1(x)
    x = self.cnn2(x)
    x = torch.relu(x)
    x = self.maxpool2(x)
    x=x.view(x.size(0),-1)
    x = self.fc1(x)
    return x

  def activations(self,x):  #outputs activation this is not necessary just for fun
    z1 = self.cnn1(x)
    a1 = torch.relu(z1)
    out = self.maxpool1(a1)

    z2 = self.cnn2(out)
    a2 = torch.relu(z2)
    out = self.maxpool2(a2)
    out=out.view(out.size(0),-1)
    return z1,a1,z2,a2,out


model=CNN(2,1)
model.state_dict()

plot_channels(model.state_dict()['cnn1.weight'])
plot_channels(model.state_dict()['cnn2.weight'])
criterion=nn.CrossEntropyLoss()
learning_rate=0.001

optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=10)
validation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=20)

n_epochs = 10
cost_list = []
accuracy_list = []
N_test = len(validation_dataset)
cost = 0
for epoch in range(n_epochs):
  cost = 0
  for x,y in train_loader:
    optimizer.zero_grad()
    z = model(x)
    loss = criterion(z,y)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    cost+=loss.item()
  cost_list.append(cost)

  correct = 0
  for x_test,y_test in validation_loader:
    z = model(x_test)
    _,yhat = torch.max(z.data,1)
    correct += (yhat==y_test).sum().item()
  accuracy = correct/N_test
  accuracy_list.append(accuracy)

fig,ax1 = plt.subplots()
color = 'tab:red'

ax1.plot(cost_list,color=color)
ax1.set_xlabel('epoch',color=color)
ax1.set_ylabel('total_loss',color=color)
ax1.tick_params(axis='y', color=color)

ax2 = ax1.twinx()
color = 'tab:blue'
ax2.set_ylabel('accuracy', color=color)
ax2.plot( accuracy_list, color=color)
ax2.tick_params(axis='y', labelcolor=color)
fig.tight_layout()

plot_channels(model.state_dict()['cnn1.weight'])
plot_channels(model.state_dict()['cnn2.weight'])
out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))
out=model.activations(train_dataset[0][0].view(1,1,11,11))

plot_activations(out[0],number_rows=1,name=" feature map")
plt.show()
plot_activations(out[2],number_rows=1,name="2nd feature map")
plt.show()
plot_activations(out[3],number_rows=1,name="first feature map")
plt.show()

out1=out[4][0].detach().numpy()
out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy()
plt.subplot(2, 1, 1)
plt.plot( out1, 'b')
plt.title('Flatted Activation Values  ')
plt.ylabel('Activation')
plt.xlabel('index')
plt.subplot(2, 1, 2)
plt.plot(out0, 'r')
plt.xlabel('index')
plt.ylabel('Activation')

"""# Convolutional Neural Network with Small Images"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import matplotlib.pylab as plt
import numpy as np

def plot_channels(W):
  n_out = W.shape[0]
  n_in = W.shape[1]
  w_min = W.min().item()
  w_max = W.max().item()
  fig,axes = plt.subplots(n_out,n_in)
  fig.subplots_adjust(hspace=0.1)
  out_index = 0
  in_index = 0

  for ax in axes.flat:
    if in_index > n_in-1:
      out_index = out_index + 1
      in_index = 0
    ax.imshow(W[out_index, in_index, :, :], vmin=w_min, vmax=w_max, cmap='seismic')
    ax.set_yticklabels([])
    ax.set_xticklabels([])
    in_index = in_index + 1

  plt.show()

def plot_parameters(W,number_rows=1,name="",i=0):
  W = W.data[:,i,:,:]
  n_filters = W.shape[0]
  w_min = W.min().item()
  w_max = W.max().item()
  fig,axes = plt.subplots(number_rows,n_filters // number_rows)
  fig.subplots_adjust(hspace=0.4)

  for i,ax in enumerate(axes.flat):
    if i < n_filters:
      ax.set_xlabel("kernel:{0}".format(i + 1))
      ax.imshow(W[i, :], vmin=w_min, vmax=w_max, cmap='seismic')
      ax.set_xticks([])
      ax.set_yticks([])
  plt.suptitle(name, fontsize=10)
  plt.show()

def plot_activations(A,number_rows=1,name="",i=0):
  A = A[0,:,:,:].detach().numpy()
  n_activations = A.shape[0]
  A_min = A.min().item()
  A_max = A.max().item()
  fig, axes = plt.subplots(number_rows, n_activations // number_rows)
  fig.subplots_adjust(hspace = 0.4)

  for i,ax in enumerate(axes.flat):
    if i < n_activations:
      ax.set_xlabel("activation:{0}".format(i + 1))
      ax.imshow(A[i, :], vmin=A_min, vmax=A_max, cmap='seismic')
      ax.set_xticks([])
      ax.set_yticks([])
  plt.show()

IMAGE_SIZE = 16

composed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])
def show_data(data_sample):
  plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')
  plt.title('y = '+ str(data_sample[1]))

train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=composed)
validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=composed)

class CNN(nn.Module):
  def __init__(self,out_1 = 16,out_2=32):
    super(CNN,self).__init__()
    self.cnn1 = nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=5,padding=2)
    self.maxpool1 = nn.MaxPool2d(kernel_size=2)
    self.cnn2 = nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=5,padding=2)
    self.maxpool2 = nn.MaxPool2d(kernel_size=2)
    self.fc1 = nn.Linear(out_2*4*4,10)

  def forward(self,x):
    x = self.cnn1(x)
    x = torch.relu(x)
    x = self.maxpool1(x)
    x = self.cnn2(x)
    x = torch.relu(x)
    x = self.maxpool2(x)
    x = x.view(x.size(0),-1)
    x = self.fc1(x)
    return x

  def activations(self,x):
    z1 = self.cnn1(x)
    a1 = torch.relu(z1)
    out = self.maxpool1(a1)

    z2 = self.cnn2(out)
    a2 = torch.relu(z2)
    out1 = self.maxpool2(a2)
    out = out.view(out.size(0),-1)
    return z1,a1,z2,a2,out1,out

model = CNN(out_1=16,out_2=32)

plot_parameters(model.state_dict()['cnn1.weight'], number_rows=4, name="1st layer kernels before training ")
plot_parameters(model.state_dict()['cnn2.weight'], number_rows=4, name='2nd layer kernels before training' )

criterion = nn.CrossEntropyLoss()
learning_rate = 0.1
optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=100)
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)

n_epochs = 3
cost_list = []
accuracy_list = []
N_test = len(validation_dataset)
COST = 0

def train_model(n_epochs):
  for epoch in range(n_epochs):
    COST = 0
    for x,y in train_loader:
      optimizer.zero_grad()
      z = model(x)
      loss = criterion(z,y)
      loss.backward()
      optimizer.step()
      COST += loss.data

    cost_list.append(COST)
    correct = 0
    for x_test,y_test in validation_loader:
      z = model(x_test)
      _,yhat = torch.max(z.data, 1)
      correct += (yhat == y_test).sum().item()
    accuracy = correct / N_test
    accuracy_list.append(accuracy)

train_model(n_epochs)
fig, ax1 = plt.subplots()
color = 'tab:red'
ax1.plot(cost_list, color=color)
ax1.set_xlabel('epoch', color=color)
ax1.set_ylabel('Cost', color=color)
ax1.tick_params(axis='y', color=color)

ax2 = ax1.twinx()
color = 'tab:blue'
ax2.set_ylabel('accuracy', color=color)
ax2.set_xlabel('epoch', color=color)
ax2.plot( accuracy_list, color=color)
ax2.tick_params(axis='y', color=color)
fig.tight_layout()

plot_channels(model.state_dict()['cnn1.weight'])
plot_channels(model.state_dict()['cnn2.weight'])

show_data(train_dataset[1])

out = model.activations(train_dataset[1][0].view(1, 1, IMAGE_SIZE, IMAGE_SIZE))
plot_activations(out[0], number_rows=4, name="Output after the 1st CNN")
plot_activations(out[1], number_rows=4, name="Output after the 1st Relu")
plot_activations(out[2], number_rows=32 // 4, name="Output after the 2nd CNN")
plot_activations(out[3], number_rows=4, name="Output after the 2nd Relu")

show_data(train_dataset[2])
out = model.activations(train_dataset[2][0].view(1, 1, IMAGE_SIZE, IMAGE_SIZE))
plot_activations(out[0], number_rows=4, name="Output after the 1st CNN")
plot_activations(out[1], number_rows=4, name="Output after the 1st Relu")
plot_activations(out[2], number_rows=32 // 4, name="Output after the 2nd CNN")
plot_activations(out[3], number_rows=4, name="Output after the 2nd Relu")

count = 0
for x, y in torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=1):
    z = model(x)
    _, yhat = torch.max(z, 1)
    if yhat != y:
        show_data((x, y))
        plt.show()
        print("yhat: ",yhat)
        count += 1
    if count >= 5:
        break

"""# Convolutional Neural Network with Batch-Normalization"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import matplotlib.pylab as plt
import numpy as np

IMAGE_SIZE = 16
def show_data(data_sample):
  plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')
  plt.title('y = '+ str(data_sample[1]))

composed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])
train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=composed)
validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=composed)

class CNN(nn.Module):
  def __init__(self,out_1=16,out_2=32):
    super(CNN,self).__init__()
    self.cnn1 = nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=5,padding=2)
    self.maxpool1 = nn.MaxPool2d(kernel_size=2)
    self.cnn2 = nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=5,stride=1,padding=2)
    self.maxpool2=nn.MaxPool2d(kernel_size=2)
    self.fc1 = nn.Linear(out_2*4*4,10)

  def forward(self,x):
    x = self.cnn1(x)
    x = torch.relu(x)
    x = self.maxpool1(x)
    x = self.cnn2(x)
    x = torch.relu(x)
    x = self.maxpool2(x)
    x = x.view(x.size(0), -1)
    x = self.fc1(x)
    return x

class CNN_batch(nn.Module):
  def __init__(self,out_1=16,out_2=32,number_of_classes=10):
    super(CNN_batch,self).__init__()
    self.cnn1 = nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=5,padding=2)
    self.conv1_bn = nn.BatchNorm2d(out_1)
    self.maxpool1 = nn.MaxPool2d(kernel_size=2)

    self.cnn2 = nn.Conv2d(in_channels =out_1,out_channels=out_2,kernel_size=5,stride=1,padding=2)
    self.conv2_bn = nn.BatchNorm2d(out_2)
    self.maxpool2 = nn.MaxPool2d(kernel_size=2)

    self.fc1 = nn.Linear(out_2*4*4,number_of_classes)
    self.bn_fc1 = nn.BatchNorm1d(10)

  def forward(self,x):
    x = self.cnn1(x)
    x = self.conv1_bn(x)
    x = torch.relu(x)
    x = self.maxpool1(x)
    x = self.cnn2(x)
    x = self.conv2_bn(x)
    x = torch.relu(x)
    x = self.maxpool2(x)
    x = x.view(x.size(0),-1)
    x = self.fc1(x)
    x = self.bn_fc1(x)
    return x

def train_model(model,train_loader,validation_loader,optimizer,n_epochs=4):
  N_test = len(validation_dataset)
  accuracy_list = []
  loss_list = []
  for epoch in range(n_epochs):
    for x,y in train_loader:
      model.train()
      optimizer.zero_grad()
      z = model(x)
      loss = criterion(z,y)
      loss.backward()
      optimizer.step()
      loss_list.append(loss.data)

    correct = 0

    for x_test,y_test in validation_loader:
      model.eval()
      z = model(x_test)
      _,yhat = torch.max(z.data,1)
      correct += (yhat == y_test).sum().item()
    accuracy = correct / N_test
    accuracy_list.append(accuracy)
  return accuracy_list, loss_list

model = CNN(out_1=16, out_2=32)
criterion = nn.CrossEntropyLoss()
learning_rate = 0.1
optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)

model_batch=CNN_batch(out_1=16, out_2=32)
criterion = nn.CrossEntropyLoss()
learning_rate = 0.1
optimizer = torch.optim.SGD(model_batch.parameters(), lr = learning_rate)

# Train the model
accuracy_list_normal, loss_list_normal=train_model(model=model,n_epochs=10,train_loader=train_loader,validation_loader=validation_loader,optimizer=optimizer)
accuracy_list_batch, loss_list_batch=train_model(model=model_batch,n_epochs=10,train_loader=train_loader,validation_loader=validation_loader,optimizer=optimizer)

plt.plot(loss_list_normal, 'b',label='loss normal cnn ')
plt.plot(loss_list_batch,'r',label='loss batch cnn')
plt.xlabel('iteration')
plt.title("loss")
plt.legend()

plt.plot(accuracy_list_normal, 'b',label=' normal CNN')
plt.plot(accuracy_list_batch,'r',label=' CNN with Batch Norm')
plt.xlabel('Epoch')
plt.title("Accuracy ")
plt.legend()
plt.show()

"""# Convolutional Neural Network for Anime Image Classification"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader,Dataset
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os
import zipfile
from PIL import Image

import io
import requests

def load_images_from_zip(zip_file):
  with zipfile.ZipFile(zip_file,'r') as zip_ref:
    images = {'anastasia': [], 'takao': []}
    for file_name in zip_ref.namelist():
      if file_name.startswith('anastasia') and file_name.endswith('.jpg'):
        with zip_ref.open(file_name) as file:
          img = Image.open(file).convert('RGB')
          images['anastasia'].append(np.array(img))
      elif file_name.startswith('takao') and file_name.endswith('.jpg'):
        with zip_ref.open(file_name) as file:
          img = Image.open(file).convert('RGB')
          images['takao'].append(np.array(img))
  return images

zip_file_url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/xZQHOyN8ONT92kH-ASb4Pw/data.zip'
response = requests.get(zip_file_url)
zip_file_bytes = io.BytesIO(response.content)

images = load_images_from_zip(zip_file_bytes)

print("Number of images of Anastasia:", len(images['anastasia']))
print("Number of images of Takao:", len(images['takao']))

def plot_images(images,title):
  fig,axes = plt.subplots(5,10,figsize=(10,5))
  fig.suptitle(title,fontsize=16)
  axes = axes.flatten()
  for img,ax in zip(images,axes):
    ax.imshow(img)
    ax.axis('off')
  plt.tight_layout()
  plt.show()

plot_images(images['anastasia'], 'Anastasia Images')
plot_images(images['takao'], 'Takao Images')

class AnimeDataset(Dataset):
  def __init__(self,images,transform=None,classes=None):
    self.images = []
    self.labels = []
    self.transform = transform
    self.classes = classes

    for label,class_name in enumerate(self.classes):
      for img in images[class_name]:
        self.images.append(img)
        self.labels.append(label)

  def __len__(self):
    return len(self.images)

  def __getitem__(self,idx):
    image = Image.fromarray(self.images[idx])
    label = self.labels[idx]

    if self.transform:
      image = self.transform(image)
    return image,label

transform = transforms.Compose([
  transforms.Resize((64, 64)),
  transforms.ToTensor(),
  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

dataset = AnimeDataset(images, transform=transform, classes=['anastasia', 'takao'])

from sklearn.model_selection import train_test_split
from torch.utils.data.sampler import SubsetRandomSampler

seed = 42
np.random.seed(seed)
torch.manual_seed(seed)

indices = list(range(len(dataset)))
train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=seed)
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

train_loader = DataLoader(dataset,batch_size=8,sampler=train_sampler)
val_loader = DataLoader(dataset,batch_size=20,sampler=val_sampler)
print("Train size:", len(train_indices))
print("Validation size:", len(val_indices))

import torch.nn as nn
import torch.nn.functional as F

class AnimeCNN(nn.Module):
  def __init__(self):
    super(AnimeCNN,self).__init__()
    self.conv1 = nn.Conv2d(3,32,3,1,padding=1)
    self.conv2 = nn.Conv2d(32,64,3,1,padding=1)
    self.pool = nn.MaxPool2d(2,2)
    self.fc1 = nn.Linear(64*16*16,128)
    self.fc2 = nn.Linear(128,2)

  def forward(self,x):
    x = self.pool(F.relu(self.conv1(x)))
    x = self.pool(F.relu(self.conv2(x)))
    x = x.view(-1, 64 * 16 * 16)
    x = F.relu(self.fc1(x))
    x = self.fc2(x)
    return x
model = AnimeCNN()
input_tensor = torch.randn(1, 3, 64, 64)

def print_size(module, input, output):
  print(f"{module.__class__.__name__} output size: {output.size()}")

hooks = []
for layer in model.children():
  hook = layer.register_forward_hook(print_size)
  hooks.append(hook)

with torch.no_grad():
  output = model(input_tensor)

for hook in hooks:
    hook.remove()

import torch.optim as optim
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(),lr=0.001)

import matplotlib.pyplot as plt
import torch

# Training loop
num_epochs = 5
train_losses = []
val_losses = []

for epoch in range(num_epochs):
  model.train()
  running_loss = 0.0
  for i,data in enumerate(train_loader, 0):
    inputs , labels = data
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = criterion(outputs,labels)
    loss.backward()
    optimizer.step()
    running_loss += loss.item()

  train_loss = running_loss / len(train_loader)
  train_losses.append(train_loss)

  model.eval()
  val_loss = 0.0

  with torch.no_grad():
    for data in val_loader:
      inputs , labels = data
      outputs = model(inputs)
      loss = criterion(outputs,labels)
      val_loss += loss.item()
  val_loss = val_loss / len(val_loader)
  val_losses.append(val_loss)
  print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')
print('Finished Training')

plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss', linestyle='--')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.title('Training and Validation Loss')
plt.show()

import matplotlib.pyplot as plt
import numpy as np
import torch
#要用img/2 + 0.5是因為transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
def imshow(img,ax):
  img = img/2+0.5
  npimg = img.numpy()
  ax.imshow(np.transpose(npimg, (1, 2, 0))) #圖片維度 把維度從 (C, H, W) 轉成 (H, W, C)。
  ax.axis('off')

model.eval()
data_iter = iter(val_loader)
images, labels = next(data_iter)
outputs = model(images)
_, predicted = torch.max(outputs, 1)

num_images = len(images)
num_cols = 10
num_rows = 2

fig,axs = plt.subplots(num_rows,num_cols*2,figsize=(20, num_rows))
for idx in range(num_images):
  row = idx // num_cols
  col = (idx % num_cols) * 2
  imshow(images[idx].cpu(), axs[row, col])
  axs[row,col+1].text(0.5,0.5,f"Actual: {labels[idx].item()}\nPredicted: {predicted[idx].item()}",horizontalalignment='center', verticalalignment='center', fontsize=12)
  axs[row, col + 1].axis('off')

for idx in range(num_images, num_rows * num_cols):
  row = idx // num_cols
  col = (idx % num_cols) * 2
  axs[row, col].axis('off')
  axs[row, col + 1].axis('off')

plt.tight_layout()
plt.show()

correct = 0
total = 0

with torch.no_grad():
  for data in val_loader:
    images,labels = data
    outputs = model(images)
    _,predicted = torch.max(outputs.data, 1)
    total += labels.size(0)
    correct += (predicted == labels).sum().item()
    print(f'correct: {correct}, total: {total}')
print(f'Validation Accuracy: {100 * correct / total:.2f}%')

